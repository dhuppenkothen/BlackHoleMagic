{
 "metadata": {
  "name": "",
  "signature": "sha256:af05c1d9704f17af4193f04fb91da465cb8e63017f14218f2b57b579ae671ed0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Extraction\n",
      "\n",
      "We're gonna need some features, so let's make some!\n",
      "\n",
      "For now, we'll have the following features to play with:\n",
      "- mean count rate `fmu`\n",
      "- variance in the total light curve `fvar`\n",
      "- integrated power in 4 bands: \n",
      "    - PA: 0.0039-0.031 Hz, \n",
      "    - PB: 0.031-0.25 Hz, \n",
      "    - PC: 0.25-2.0 Hz and \n",
      "    - PD: 2.0-16.0 Hz\n",
      "    - power colours: PC/PA and PB/PD\n",
      "- frequency at which maximum power is observed\n",
      "- hardness ratios in 2 bands (processed data only, NOT Standard 1 data)\n",
      "    - (2-6keV)/(2-13keV)\n",
      "    - (9-20kev)/(2-13keV)\n",
      "\n",
      "For now, we're returning the mean hardness ratio within a segment, but we could\n",
      "also return all hardness ratio points as a vector. This may be more descriptive,\n",
      "but also big.\n",
      "\n",
      "\n",
      "First, we'll need to load the data. This assumes that you've saved a pickle file with a list of $N$ light curves and states, `[[lc1, state1], [lc2, state2], ..., [lc_n, state_n]]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import cPickle as pickle\n",
      "import pandas as pd\n",
      "\n",
      "import powerspectrum\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/__init__.py:1312: UserWarning:  This call to matplotlib.use() has no effect\n",
        "because the backend has already been chosen;\n",
        "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\n",
        "  warnings.warn(_use_error_msg)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_clean_label_125ms.dat\")\n",
      "d_all = pickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break down the frequencies of states in these light curves. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Number of light curves:  \" + str(len(d_all)))\n",
      "states = [d[1] for d in d_all]\n",
      "st = pd.Series(states)\n",
      "st.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of light curves:  420\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "chi2      61\n",
        "chi4      43\n",
        "delta     40\n",
        "alpha     36\n",
        "gamma     30\n",
        "beta      28\n",
        "nu        27\n",
        "mu        27\n",
        "rho       24\n",
        "phi       24\n",
        "theta     22\n",
        "chi1      20\n",
        "lambda    17\n",
        "chi3      15\n",
        "kappa      6\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that the classes are *fairly* evenly distributed, with $\\chi_2$ having the most light curves in their samples, and $\\kappa$ being undersampled. \n",
      "\n",
      "In the next step, we'll pick out training set, validation set and test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## total number of light curves\n",
      "n_lcs = len(d_all)\n",
      "\n",
      "## shuffle list of light curves\n",
      "np.random.shuffle(d_all)\n",
      "\n",
      "train_frac = 0.5\n",
      "validation_frac = 0.25\n",
      "test_frac = 0.25\n",
      "\n",
      "## let's pull out light curves for three data sets into different variables.\n",
      "d_all_train = d_all[:int(train_frac*n_lcs)]\n",
      "d_all_val = d_all[int(train_frac*n_lcs):int((train_frac + validation_frac)*n_lcs)]\n",
      "d_all_test = d_all[int((train_frac + validation_frac)*n_lcs):]\n",
      "\n",
      "## Let's print some information about the three sets.\n",
      "print(\"There are %i light curves in the training set.\"%len(d_all_train))\n",
      "print(\"There are %i light curves in the validation set.\"%len(d_all_val))\n",
      "print(\"There are %i light curves in the test set.\"%len(d_all_test))\n",
      "for da,n in zip([d_all_train, d_all_val, d_all_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    states = [d[1] for d in da]\n",
      "    st = pd.Series(states)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 210 light curves in the training set.\n",
        "There are 105 light curves in the validation set.\n",
        "There are 105 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      28\n",
        "delta     25\n",
        "chi4      25\n",
        "mu        16\n",
        "gamma     15\n",
        "nu        14\n",
        "theta     14\n",
        "rho       14\n",
        "beta      14\n",
        "phi       13\n",
        "alpha     11\n",
        "chi1      10\n",
        "chi3       5\n",
        "lambda     4\n",
        "kappa      2\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "alpha     17\n",
        "chi2      16\n",
        "gamma      9\n",
        "delta      9\n",
        "nu         6\n",
        "beta       6\n",
        "chi3       6\n",
        "phi        6\n",
        "mu         5\n",
        "chi4       5\n",
        "lambda     4\n",
        "theta      4\n",
        "rho        4\n",
        "chi1       4\n",
        "kappa      4\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      17\n",
        "chi4      13\n",
        "lambda     9\n",
        "alpha      8\n",
        "beta       8\n",
        "nu         7\n",
        "gamma      6\n",
        "delta      6\n",
        "rho        6\n",
        "mu         6\n",
        "chi1       6\n",
        "phi        5\n",
        "theta      4\n",
        "chi3       4\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a next step, we can make light curve segments, overlapping or not, to then pass to feature extraction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## This function is also in grs1915_utils.py!\n",
      "def extract_segments(d_all, seg_length = 256., overlap=64.):\n",
      "    \"\"\" Extract light curve segmens from a list of light curves. \n",
      "        Each element in the list is a list with two elements: \n",
      "        - an array that contains the light curve in three energy bands \n",
      "        (full, low energies, high energies) and \n",
      "        - a string containing the state of that light curve.\n",
      "        \n",
      "        The parameters are \n",
      "        - seg_length: the length of each segment. Bits of data at the end of a light curve\n",
      "        that are smaller than seg_length will not be included. \n",
      "        - overlap: This is actually the interval between start times of individual segments,\n",
      "        i.e. by default light curves start 64 seconds apart. The actual overlap is \n",
      "        seg_length-overlap\n",
      "    \"\"\"\n",
      "    segments, labels = [], [] ## labels for labelled data\n",
      "        \n",
      "    for i,d_seg in enumerate(d_all):\n",
      "        \n",
      "        ## data is an array in the first element of d_seg\n",
      "        data = d_seg[0]\n",
      "        ## state is a string in the second element of d_seg\n",
      "        state = d_seg[1]\n",
      "\n",
      "        ## compute the intervals between adjacent time bins\n",
      "        dt_data = data[1:,0] - data[:-1,0]\n",
      "        dt = np.min(dt_data)\n",
      "        #print(\"dt: \" + str(dt))\n",
      "        \n",
      "        ## compute the number of time bins in a segment\n",
      "        nseg = seg_length/dt\n",
      "        ## compute the number of time bins to start of next segment\n",
      "        noverlap = overlap/dt\n",
      "        \n",
      "        istart = 0\n",
      "        iend = nseg\n",
      "        j = 0\n",
      "     \n",
      "        while iend <= len(data):\n",
      "            dtemp = data[istart:iend]\n",
      "            segments.append(dtemp)\n",
      "            labels.append(state)\n",
      "            istart += noverlap\n",
      "            iend += noverlap\n",
      "            j+=1\n",
      "        \n",
      "    return segments, labels\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seg_train, labels_train = extract_segments(d_all_train, seg_length=256., overlap=64.)\n",
      "seg_val, labels_val = extract_segments(d_all_val, seg_length=256., overlap=64.)\n",
      "seg_test, labels_test = extract_segments(d_all_test, seg_length=256., overlap=64.)\n",
      "\n",
      "## Let's print some details on the different segment data sets\n",
      "print(\"There are %i segments in the training set.\"%len(seg_train))\n",
      "print(\"There are %i segments in the validation set.\"%len(seg_val))\n",
      "print(\"There are %i segments in the test set.\"%len(seg_test))\n",
      "for la,n in zip([labels_train, labels_val, labels_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    st = pd.Series(la)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 4798 segments in the training set.\n",
        "There are 2553 segments in the validation set.\n",
        "There are 2449 segments in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      668\n",
        "chi4      489\n",
        "beta      430\n",
        "mu        421\n",
        "delta     420\n",
        "rho       362\n",
        "phi       338\n",
        "theta     334\n",
        "chi1      327\n",
        "gamma     298\n",
        "alpha     230\n",
        "nu        214\n",
        "chi3      161\n",
        "lambda     62\n",
        "kappa      44\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      461\n",
        "alpha     335\n",
        "gamma     235\n",
        "delta     177\n",
        "phi       172\n",
        "chi3      159\n",
        "nu        147\n",
        "chi1      144\n",
        "kappa     131\n",
        "beta      130\n",
        "mu        121\n",
        "rho       106\n",
        "theta      95\n",
        "chi4       80\n",
        "lambda     60\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      394\n",
        "chi4      317\n",
        "lambda    246\n",
        "chi1      222\n",
        "gamma     209\n",
        "beta      200\n",
        "alpha     151\n",
        "mu        145\n",
        "phi       136\n",
        "rho       106\n",
        "delta     104\n",
        "nu         79\n",
        "theta      75\n",
        "chi3       65\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now, we can think about actual feature extraction.\n",
      "\n",
      "Let's make some functions to extract individual features from a segment. We can then combine them at will. \n",
      "\n",
      "First, we're going to extract time series features derived from the light curve and the periodogram: the mean, median and variance of the light curve, the integrated power in 4 power spectral bands and the two power colours. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## boundaries for power bands\n",
      "pcb = {\"pa_min\":0.0039, \"pa_max\":0.031, \n",
      "       \"pb_min\":0.031, \"pb_max\":0.25,\n",
      "       \"pc_min\":0.25, \"pc_max\":2.0,\n",
      "       \"pd_min\":2.0, \"pd_max\":16.0}\n",
      "\n",
      "\n",
      "def timeseries_features(seg):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    fmean = np.mean(counts)\n",
      "    fmedian = np.median(counts)\n",
      "    fvar = np.var(counts)\n",
      "    return fmean, fmedian, fvar\n",
      "\n",
      "\n",
      "def psd_features(seg, pcb):\n",
      "    \n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "    fmax_ind = np.where(ps.ps[1:] == np.max(ps.ps[1:]))[0]+1\n",
      "    maxfreq = ps.freq[fmax_ind[0]]\n",
      "\n",
      "    ## find power in spectral bands for power-colours\n",
      "    pa_min_freq = ps.freq.searchsorted(pcb[\"pa_min\"])\n",
      "    pa_max_freq = ps.freq.searchsorted(pcb[\"pa_max\"])\n",
      "\n",
      "    pb_min_freq = ps.freq.searchsorted(pcb[\"pb_min\"])\n",
      "    pb_max_freq = ps.freq.searchsorted(pcb[\"pb_max\"])\n",
      "\n",
      "    pc_min_freq = ps.freq.searchsorted(pcb[\"pc_min\"])\n",
      "    pc_max_freq = ps.freq.searchsorted(pcb[\"pc_max\"])\n",
      "\n",
      "    pd_min_freq = ps.freq.searchsorted(pcb[\"pd_min\"])\n",
      "    pd_max_freq = ps.freq.searchsorted(pcb[\"pd_max\"])\n",
      "\n",
      "    psd_a = np.sum(ps.ps[pa_min_freq:pa_max_freq])\n",
      "    psd_b = np.sum(ps.ps[pb_min_freq:pb_max_freq])\n",
      "    psd_c = np.sum(ps.ps[pc_min_freq:pc_max_freq])\n",
      "    psd_d = np.sum(ps.ps[pd_min_freq:pd_max_freq])\n",
      "    pc1 = np.sum(ps.ps[pc_min_freq:pc_max_freq])/np.sum(ps.ps[pa_min_freq:pa_max_freq])\n",
      "    pc2 = np.sum(ps.ps[pb_min_freq:pb_max_freq])/np.sum(ps.ps[pd_min_freq:pd_max_freq])\n",
      "    \n",
      "    return maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we'll write functions for hardness ratio based features.\n",
      "Note that the limits for the hardness ratio are derived from the *labelled data only*! We might need to check this again when we consider the whole sample!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr_limits = [[0.292, 0.820],[0.046, 0.708]]\n",
      "\n",
      "def hr_maps(seg, hr_limits, bins=30):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    h, xedges, yedges = np.histogram2d(hr1, hr2, bins=bins, \n",
      "                                       range=hr_limits)\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "    \n",
      "    return xedges, yedges, h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use this code to extract some features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_features(seg):\n",
      "    features = []\n",
      "    for s in seg_train:\n",
      "        features_temp = []\n",
      "        fmean, fmedian, fvar = timeseries_features(s)\n",
      "        features_temp.extend([fmean, fmedian, fvar])\n",
      "\n",
      "        maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2 = psd_features(s, pcb)\n",
      "        features_temp.extend([maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2])\n",
      "\n",
      "        xedges, yedges, h = hr_maps(s, hr_limits, bins=30)\n",
      "        features_temp.extend(h.flatten())\n",
      "\n",
      "        features.append(features_temp)\n",
      "        \n",
      "    return np.array(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train = make_features(seg_train)\n",
      "features_val = make_features(seg_val)\n",
      "features_test = make_features(seg_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's save our features in a pickle object for easy extraction in the next step. We're going to make a dictionary with keywords \"train\", \"test\", \"val\", each of which has a training vector and the corresponding labels. Rows are samples, columns features. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labelled_features = {\"train\": [features_train, labels_train],\n",
      "                     \"val\": [features_val, labels_val],\n",
      "                     \"test\": [features_test, labels_test]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_labelled_features.dat\", \"w\")\n",
      "pickle.dump(labelled_features, f)\n",
      "f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "============================================================\n",
      "## Old Feature Extraction\n",
      "\n",
      "In case I still need it at some point.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## set clean_only=True if you only want to extract the sample\n",
      "## with labels, not the full sample\n",
      "\n",
      "def extract_features(d_all, pcb, tseg=256., overlap_time=64., clean_only=True, hr=True, output_lc=True):\n",
      "    \"\"\"\n",
      "    Extract features from a set of light curves d_all.\n",
      "    \n",
      "    pcb defines the boundaries for the frequency bands used for the power colours.\n",
      "    \n",
      "    If clean_only is true, extract only features from data points\n",
      "    where there is a label (otherwise extract features from entire\n",
      "    data set).\n",
      "    \n",
      "    if hr is true, assume that data is given in several energy bands\n",
      "    and we can extract hardness ratios. \n",
      "    \n",
      "    if output_lc is True, return light curves and power spectra along with\n",
      "    features (ONLY DO THIS FOR DEBUGGING ON SMALL DATA SETS, OTHERWISE YOU'LL\n",
      "    KILL YOUR MEMORY! You've been warned).\n",
      "    \n",
      "    \"\"\"\n",
      "    ## now make power spectra and extract features\n",
      "    ## in the following lists:\n",
      "    fmu, fvar = [], [] ## count rate mean and variance\n",
      "    fmax = [] ## frequency where power in the entire band is maximum\n",
      "    psd_a, psd_b, psd_c, psd_d = [], [], [], [] ## power colour bands\n",
      "    pc1, pc2 = [], [] ## power colours\n",
      "    labels = [] ## labels for labelled data\n",
      "    if output_lc:\n",
      "        lc_all = [] ## all light curves (includes power spectra and features)\n",
      "    \n",
      "    if hr:\n",
      "        hr1, hr2, hr1_mean, hr2_mean, hr1_var, hr2_var = [], [], [], [], [], []\n",
      "    \n",
      "    for i,d_seg in enumerate(d_all):\n",
      "        \n",
      "   \n",
      "        data = d_seg[0]\n",
      "        #print(data)\n",
      "        state = d_seg[1]\n",
      "\n",
      "        dt_data = data[1:,0] - data[:-1,0]\n",
      "        dt = np.min(dt_data)\n",
      "        print(\"dt: \" + str(dt))\n",
      "        nseg = tseg/dt\n",
      "        noverlap = overlap_time/dt\n",
      "        \n",
      "        istart = 0\n",
      "        iend = nseg\n",
      "        j = 0\n",
      "     \n",
      "        if clean_only == False or (clean_only == True and state is not None):\n",
      "            #print(state)\n",
      "            while iend <= len(data):\n",
      "                dtemp = data[istart:iend]\n",
      "                times = dtemp[:,0]\n",
      "                counts = dtemp[:,1]\n",
      "                if hr:\n",
      "                    low_counts = dtemp[:,2]\n",
      "                    high_counts = dtemp[:,3]\n",
      "                    hr1_temp = np.mean(low_counts/counts)\n",
      "                    hr2_temp = np.mean(high_counts/counts)\n",
      "                    hr1_var_temp = np.var(low_counts/counts)\n",
      "                    hr2_var_temp = np.var(high_counts/counts)\n",
      "                lc = lightcurve.Lightcurve(times, counts=counts)\n",
      "                lc.fmu = np.mean(counts)\n",
      "                lc.fvar = np.var(counts)\n",
      "                lc.state = state\n",
      "                try:\n",
      "                    ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "                except:\n",
      "                    print(\"Exception! boo!\")\n",
      "                    istart += noverlap\n",
      "                    iend += noverlap\n",
      "                    continue\n",
      "                #lc.ps = ps\n",
      "\n",
      "                if len(ps.freq) < 1024:\n",
      "                    print(\"i: \" + str(i))\n",
      "                    print(\"j: \" + str(j))\n",
      "                    print(len(ps.freq))\n",
      "                    print(ps.freq[0])\n",
      "                    print(ps.freq[-1])\n",
      "\n",
      "                ## find frequency with maximum power, exclude 0th frequency\n",
      "                fmax_ind = np.where(ps.ps[1:] == np.max(ps.ps[1:]))[0]+1\n",
      "                lc.maxfreq = ps.freq[fmax_ind[0]]\n",
      "\n",
      "                ## find power in spectral bands for power-colours\n",
      "                pa_min_freq = ps.freq.searchsorted(pcb[\"pa_min\"])\n",
      "                pa_max_freq = ps.freq.searchsorted(pcb[\"pa_max\"])\n",
      "\n",
      "                pb_min_freq = ps.freq.searchsorted(pcb[\"pb_min\"])\n",
      "                pb_max_freq = ps.freq.searchsorted(pcb[\"pb_max\"])\n",
      "\n",
      "                pc_min_freq = ps.freq.searchsorted(pcb[\"pc_min\"])\n",
      "                pc_max_freq = ps.freq.searchsorted(pcb[\"pc_max\"])\n",
      "\n",
      "                pd_min_freq = ps.freq.searchsorted(pcb[\"pd_min\"])\n",
      "                pd_max_freq = ps.freq.searchsorted(pcb[\"pd_max\"])\n",
      "\n",
      "                lc.psd_a = np.sum(ps.ps[pa_min_freq:pa_max_freq])\n",
      "                lc.psd_b = np.sum(ps.ps[pb_min_freq:pb_max_freq])\n",
      "                lc.psd_c = np.sum(ps.ps[pc_min_freq:pc_max_freq])\n",
      "                lc.psd_d = np.sum(ps.ps[pd_min_freq:pd_max_freq])\n",
      "                lc.pc1 = np.sum(ps.ps[pc_min_freq:pc_max_freq])/np.sum(ps.ps[pa_min_freq:pa_max_freq])\n",
      "                lc.pc2 = np.sum(ps.ps[pb_min_freq:pb_max_freq])/np.sum(ps.ps[pd_min_freq:pd_max_freq])\n",
      "\n",
      "                if clean_only == False or (clean_only == True and state is not None):\n",
      "                    fmu.append(lc.fmu)\n",
      "                    fvar.append(lc.fvar)\n",
      "                    labels.append(state)\n",
      "                    fmax.append(lc.maxfreq)\n",
      "\n",
      "                    psd_a.append(np.sum(ps.ps[pa_min_freq:pa_max_freq]))\n",
      "                    psd_b.append(np.sum(ps.ps[pb_min_freq:pb_max_freq]))\n",
      "                    psd_c.append(np.sum(ps.ps[pc_min_freq:pc_max_freq]))\n",
      "                    psd_d.append(np.sum(ps.ps[pd_min_freq:pd_max_freq]))\n",
      "\n",
      "                    pc1.append(np.sum(ps.ps[pc_min_freq:pc_max_freq])/np.sum(ps.ps[pa_min_freq:pa_max_freq]))\n",
      "                    pc2.append(np.sum(ps.ps[pb_min_freq:pb_max_freq])/np.sum(ps.ps[pd_min_freq:pd_max_freq]))\n",
      "\n",
      "                    if hr:\n",
      "                        hr1.append(low_counts/counts)\n",
      "                        hr2.append(high_counts/counts)\n",
      "                        hr1_mean.append(hr1_temp)\n",
      "                        hr2_mean.append(hr2_temp)\n",
      "                        hr1_var.append(hr1_var_temp)\n",
      "                        hr2_var.append(hr2_var_temp)\n",
      "                    if output_lc:\n",
      "                        lc_all.append(lc)\n",
      "                #labels.append(state)\n",
      "                #plt.figure()\n",
      "                #plt.loglog(ps.freq[1:], ps.ps[1:], lw=2, color=\"black\")\n",
      "                istart += noverlap\n",
      "                iend += noverlap\n",
      "                j+=1\n",
      "        else:\n",
      "            continue\n",
      "            \n",
      "    all_features = {\"fmu\":fmu, \"fvar\":fvar, \"labels\":labels, \"fmax\":fmax, \n",
      "                    \"psd_a\":psd_a, \"psd_b\":psd_b, \"psd_c\":psd_c, \"psd_d\":psd_d,\n",
      "                    \"pc1\":pc1, \"pc2\":pc2}\n",
      "    if output_lc:\n",
      "        all_features[\"lc_all\"] = lc_all\n",
      "    if hr:\n",
      "        all_features[\"hr1\"] = hr1\n",
      "        all_features[\"hr2\"] = hr2\n",
      "        all_features[\"hr1 mean\"] = hr1_mean\n",
      "        all_features[\"hr2 mean\"] = hr2_mean\n",
      "        all_features[\"hr1 var\"] = hr1_var\n",
      "        all_features[\"hr2 var\"] = hr2_var\n",
      "\n",
      "    return all_features\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## here are some dumb assumptions about the light curves we'll use\n",
      "tseg = 256. ## segment length\n",
      "overlap_time = 256. ## overlap in seconds\n",
      "\n",
      "af = extract_features(d_all, power_colour_boundaries, tseg=tseg, overlap_time=overlap_time, clean_only = False, output_lc=True, hr=True)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "============================================================\n",
      "## Low-Memory Feature Extraction\n",
      "\n",
      "Some code I might need in the future when I'm doing this on thousands and thousands\n",
      "of light curves, so it's here for future reference and completion. \n",
      "This code includes both parts of the data extraction and the feature extraction, so it'll\n",
      "probably need to be heavily modified by the time I get around to doing this on the full \n",
      "data set, but it's here as a start.\n",
      "\n",
      "Keeping thousands of light curves in memory quickly drains resources. \n",
      "\n",
      "Below is an implementation of the code reading in light curves and extracting\n",
      "features that's less computationally expensive by only keeping the resulting \n",
      "features in memory (light curves can be retained by setting keep_lcs = True)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features_lowmem(datafiles, pcb, bin_data=True, bin_res = 0.125, tseg=256., overlap_time=64., \\\n",
      "                            label_only=True, labels=\"clean\", hr=True, output_lc=True):\n",
      "    \"\"\"\n",
      "    Extract Observing Mode data from a list of text files with columns \n",
      "    #times \\t total count rate \\t low energy count rate \\t high energy count rate\n",
      "    \n",
      "    if bin_data=True, the data will be rebinned to the resolution specified in bin_res.\n",
      "    If label_only is True, only data with (manually determined) labels will be extracted;\n",
      "    either from the whole set of observations used in Belloni+ 2000 (labels=\"all\") or from\n",
      "    the cleaned set with only those observations where the state does not change during the \n",
      "    observation (labels=\"clean\")\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    if label_only:\n",
      "        if labels == \"clean\":\n",
      "            belloni_states = convert_belloni.main()\n",
      "            belloni_turned = turn_states(belloni_states)\n",
      "        else:\n",
      "            belloni_turned = convert_belloni_clean()\n",
      "    else:\n",
      "        belloni_turned = [None]\n",
      "    \n",
      "    all_features = {\"fmu\":[], \"fvar\":[], \"labels\":[], \"fmax\":[], \n",
      "                    \"psd_a\":[], \"psd_b\":[], \"psd_c\":[], \"psd_d\":[],\n",
      "                    \"pc1\":[], \"pc2\":[]}\n",
      "    if output_lc:\n",
      "        all_features[\"lc_all\"] = []\n",
      "    if hr:\n",
      "        all_features[\"hr1\"] = []\n",
      "        all_features[\"hr2\"] = []\n",
      "        all_features[\"hr1 mean\"] = []\n",
      "        all_features[\"hr2 mean\"] =[]\n",
      "        all_features[\"hr1 var\"] = []\n",
      "        all_features[\"hr2 var\"] = []\n",
      "    for f in datafiles:\n",
      "        print(\"I am on file \" + str(f))\n",
      "        fstring = f.split(\"_\")[1]\n",
      "        if fstring in belloni_turned:\n",
      "            state = belloni_turned[fstring]\n",
      "        else:\n",
      "            state = None\n",
      "            if label_only:\n",
      "                print(\"No class assigned to file %s. Skipping ...\"%f)\n",
      "                continue\n",
      "                \n",
      "        d = np.loadtxt(f)\n",
      "        #times = d[:,0]\n",
      "        #counts = d[:,1]\n",
      "        #plt.plot(times, counts)\n",
      "        dt_data = d[1:,0]-d[:-1,0]\n",
      "\n",
      "        dt_min = np.min(dt_data)\n",
      "        \n",
      "        #print(\"dt_min: \" + str(dt_min))\n",
      "        #nseg = int(tseg/dt_min) ## number of bins per segment\n",
      "        #noverlap = int(overlap_time/dt_min)\n",
      "        #print(\"nseg: \" + str(nseg))\n",
      "        #print(\"noverlap: \" + str(noverlap))\n",
      "\n",
      "        nbins = int(bin_res/dt_min)\n",
      "        #print(\"bin_res: \" + str(bin_res))\n",
      "        #print(\"dt_min: \" + str(dt_min))\n",
      "        #print(\"nbins: \" + str(nbins))\n",
      "\n",
      "        ### split data with breaks\n",
      "        breaks = np.where(dt_data > 0.008)[0]\n",
      "        #print(breaks)\n",
      "        if len(breaks) == 0:\n",
      "            dtemp = d\n",
      "            if bin_data:\n",
      "                #nbins = int(bin_res/tres)\n",
      "                dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "            else:\n",
      "                dshort = dtemp  \n",
      "            features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                        clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "            for k in all_features.keys():\n",
      "                all_features[k].append(features[k])\n",
      "\n",
      "\n",
      "        else:\n",
      "            for i,b in enumerate(breaks):\n",
      "                #print(\"Break in light curve at time bin %i; length of break dt = %.3f\"%(b, dt_data[b]))\n",
      "                if i == 0:\n",
      "                    if b == 0:\n",
      "                        #print(\"First break is at first time bin\")\n",
      "                        continue\n",
      "                    else:\n",
      "                        #print(\"I am extracting the data before the break\")\n",
      "                        dtemp = d[:b]\n",
      "                        if bin_data:\n",
      "                            #nbins = int(bin_res/tres)\n",
      "                            dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "                        else:\n",
      "                            dshort = dtemp\n",
      "                            \n",
      "                        print(\"dshort.shape: \" + str(dshort.shape))\n",
      "\n",
      "                        features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                    clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "                        \n",
      "                        for k in all_features.keys():\n",
      "                            all_features[k].append(features[k])\n",
      "                \n",
      "                else:\n",
      "                    #print(\"I am extracting data after break \" + str(i))\n",
      "                    dtemp = d[breaks[i-1]+1:b]\n",
      "                    if bin_data:\n",
      "                        #nbins = int(bin_res/tres)\n",
      "                        dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "                    else:\n",
      "                        dshort = dtemp\n",
      "                    print(\"dshort.shape: \" + str(dshort.shape))\n",
      "\n",
      "                    features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                                clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "                    for k in all_features.keys():\n",
      "                        all_features[k].append(features[k])\n",
      "\n",
      "\n",
      "\n",
      "            ## last segment\n",
      "            #print(\"I am computing last segment of file\")\n",
      "            dtemp = d[b+1:]\n",
      "            if bin_data:\n",
      "                #nbins = int(bin_res/tres)\n",
      "                dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "            else:\n",
      "                dshort = dtemp\n",
      "            print(\"dshort.shape: \" + str(dshort.shape))\n",
      "            features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                        clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "            for k in all_features.keys():\n",
      "                all_features[k].append(features[k])\n",
      "\n",
      "    return all_features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}