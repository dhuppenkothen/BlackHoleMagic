{
 "metadata": {
  "name": "",
  "signature": "sha256:e80c42ba5d58bdc9e465ca2e3a3cc50d4ef61cbc4f7968a84030af87600f8eab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Extraction\n",
      "\n",
      "We're gonna need some features, so let's make some!\n",
      "\n",
      "For now, we'll have the following features to play with:\n",
      "- mean count rate `fmu`\n",
      "- variance in the total light curve `fvar`\n",
      "- integrated power in 4 bands: \n",
      "    - PA: 0.0039-0.031 Hz, \n",
      "    - PB: 0.031-0.25 Hz, \n",
      "    - PC: 0.25-2.0 Hz and \n",
      "    - PD: 2.0-16.0 Hz\n",
      "    - power colours: PC/PA and PB/PD\n",
      "- frequency at which maximum power is observed\n",
      "- hardness ratios in 2 bands (processed data only, NOT Standard 1 data)\n",
      "    - (2-6keV)/(2-13keV)\n",
      "    - (9-20kev)/(2-13keV)\n",
      "\n",
      "For now, we're returning the mean hardness ratio within a segment, but we could\n",
      "also return all hardness ratio points as a vector. This may be more descriptive,\n",
      "but also big.\n",
      "\n",
      "\n",
      "First, we'll need to load the data. This assumes that you've saved a pickle file with a list of $N$ light curves and states, `[[lc1, state1], [lc2, state2], ..., [lc_n, state_n]]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import glob\n",
      "import cPickle as pickle\n",
      "import pandas as pd\n",
      "\n",
      "import powerspectrum\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/__init__.py:1312: UserWarning:  This call to matplotlib.use() has no effect\n",
        "because the backend has already been chosen;\n",
        "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\n",
        "  warnings.warn(_use_error_msg)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"../grs1915_clean_label_125ms.dat\")\n",
      "d_all = pickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break down the frequencies of states in these light curves. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Number of light curves:  \" + str(len(d_all)))\n",
      "states = [d[1] for d in d_all]\n",
      "st = pd.Series(states)\n",
      "st.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of light curves:  426\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "chi2      68\n",
        "chi4      47\n",
        "delta     40\n",
        "gamma     32\n",
        "chi1      30\n",
        "beta      28\n",
        "nu        27\n",
        "mu        27\n",
        "rho       24\n",
        "phi       24\n",
        "theta     22\n",
        "alpha     19\n",
        "lambda    17\n",
        "chi3      15\n",
        "kappa      6\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that the classes are *fairly* evenly distributed, with $\\chi_2$ having the most light curves in their samples, and $\\kappa$ being undersampled. \n",
      "\n",
      "In the next step, we'll pick out training set, validation set and test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## total number of light curves\n",
      "n_lcs = len(d_all)\n",
      "\n",
      "## shuffle list of light curves\n",
      "np.random.shuffle(d_all)\n",
      "\n",
      "train_frac = 0.6\n",
      "validation_frac = 0.2\n",
      "test_frac = 0.2\n",
      "\n",
      "## let's pull out light curves for three data sets into different variables.\n",
      "d_all_train = d_all[:int(train_frac*n_lcs)]\n",
      "d_all_val = d_all[int(train_frac*n_lcs):int((train_frac + validation_frac)*n_lcs)]\n",
      "d_all_test = d_all[int((train_frac + validation_frac)*n_lcs):]\n",
      "\n",
      "## Let's print some information about the three sets.\n",
      "print(\"There are %i light curves in the training set.\"%len(d_all_train))\n",
      "print(\"There are %i light curves in the validation set.\"%len(d_all_val))\n",
      "print(\"There are %i light curves in the test set.\"%len(d_all_test))\n",
      "for da,n in zip([d_all_train, d_all_val, d_all_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    states = [d[1] for d in da]\n",
      "    st = pd.Series(states)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 255 light curves in the training set.\n",
        "There are 85 light curves in the validation set.\n",
        "There are 86 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      40\n",
        "chi4      33\n",
        "delta     30\n",
        "chi1      23\n",
        "gamma     16\n",
        "mu        15\n",
        "nu        13\n",
        "theta     13\n",
        "rho       13\n",
        "beta      13\n",
        "lambda    12\n",
        "alpha     11\n",
        "chi3      11\n",
        "phi        8\n",
        "kappa      4\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      14\n",
        "gamma     13\n",
        "phi        9\n",
        "delta      8\n",
        "nu         7\n",
        "theta      6\n",
        "rho        5\n",
        "chi4       5\n",
        "alpha      5\n",
        "beta       4\n",
        "mu         3\n",
        "chi1       2\n",
        "chi3       2\n",
        "lambda     1\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      14\n",
        "beta      11\n",
        "mu         9\n",
        "chi4       9\n",
        "nu         7\n",
        "phi        7\n",
        "rho        6\n",
        "chi1       5\n",
        "lambda     4\n",
        "gamma      3\n",
        "theta      3\n",
        "alpha      3\n",
        "delta      2\n",
        "chi3       2\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a next step, we can make light curve segments, overlapping or not, to then pass to feature extraction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## This function is also in grs1915_utils.py!\n",
      "def extract_segments(d_all, seg_length = 256., overlap=64.):\n",
      "    \"\"\" Extract light curve segmens from a list of light curves. \n",
      "        Each element in the list is a list with two elements: \n",
      "        - an array that contains the light curve in three energy bands \n",
      "        (full, low energies, high energies) and \n",
      "        - a string containing the state of that light curve.\n",
      "        \n",
      "        The parameters are \n",
      "        - seg_length: the length of each segment. Bits of data at the end of a light curve\n",
      "        that are smaller than seg_length will not be included. \n",
      "        - overlap: This is actually the interval between start times of individual segments,\n",
      "        i.e. by default light curves start 64 seconds apart. The actual overlap is \n",
      "        seg_length-overlap\n",
      "    \"\"\"\n",
      "    segments, labels = [], [] ## labels for labelled data\n",
      "        \n",
      "    for i,d_seg in enumerate(d_all):\n",
      "        \n",
      "        ## data is an array in the first element of d_seg\n",
      "        data = d_seg[0]\n",
      "        ## state is a string in the second element of d_seg\n",
      "        state = d_seg[1]\n",
      "\n",
      "        ## compute the intervals between adjacent time bins\n",
      "        dt_data = data[1:,0] - data[:-1,0]\n",
      "        dt = np.min(dt_data)\n",
      "        #print(\"dt: \" + str(dt))\n",
      "        \n",
      "        ## compute the number of time bins in a segment\n",
      "        nseg = seg_length/dt\n",
      "        ## compute the number of time bins to start of next segment\n",
      "        noverlap = overlap/dt\n",
      "        \n",
      "        istart = 0\n",
      "        iend = nseg\n",
      "        j = 0\n",
      "     \n",
      "        while iend <= len(data):\n",
      "            dtemp = data[istart:iend]\n",
      "            segments.append(dtemp)\n",
      "            labels.append(state)\n",
      "            istart += noverlap\n",
      "            iend += noverlap\n",
      "            j+=1\n",
      "        \n",
      "    return segments, labels\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seg_train, labels_train = extract_segments(d_all_train, seg_length=1024., overlap=128.)\n",
      "seg_val, labels_val = extract_segments(d_all_val, seg_length=1024., overlap=128.)\n",
      "seg_test, labels_test = extract_segments(d_all_test, seg_length=1024., overlap=128.)\n",
      "\n",
      "## Let's print some details on the different segment data sets\n",
      "print(\"There are %i segments in the training set.\"%len(seg_train))\n",
      "print(\"There are %i segments in the validation set.\"%len(seg_val))\n",
      "print(\"There are %i segments in the test set.\"%len(seg_test))\n",
      "for la,n in zip([labels_train, labels_val, labels_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    st = pd.Series(la)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 1439 segments in the training set.\n",
        "There are 759 segments in the validation set.\n",
        "There are 675 segments in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      200\n",
        "chi4      140\n",
        "phi       130\n",
        "alpha     120\n",
        "mu        120\n",
        "gamma     110\n",
        "rho       106\n",
        "chi1      103\n",
        "beta       91\n",
        "theta      85\n",
        "delta      73\n",
        "lambda     47\n",
        "kappa      43\n",
        "chi3       39\n",
        "nu         32\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      162\n",
        "beta       76\n",
        "chi1       65\n",
        "lambda     62\n",
        "chi3       61\n",
        "gamma      49\n",
        "chi4       43\n",
        "phi        41\n",
        "delta      38\n",
        "rho        37\n",
        "theta      35\n",
        "alpha      31\n",
        "mu         25\n",
        "nu         20\n",
        "kappa      14\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2     100\n",
        "chi1      74\n",
        "beta      70\n",
        "chi4      67\n",
        "gamma     62\n",
        "mu        58\n",
        "delta     50\n",
        "alpha     47\n",
        "nu        41\n",
        "phi       32\n",
        "rho       28\n",
        "theta     25\n",
        "chi3      21\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now, we can think about actual feature extraction.\n",
      "\n",
      "Let's make some functions to extract individual features from a segment. We can then combine them at will. \n",
      "\n",
      "First, we're going to extract time series features derived from the light curve and the periodogram: the mean, median and variance of the light curve, the integrated power in 4 power spectral bands and the two power colours. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## boundaries for power bands\n",
      "pcb = {\"pa_min\":0.0039, \"pa_max\":0.031, \n",
      "       \"pb_min\":0.031, \"pb_max\":0.25,\n",
      "       \"pc_min\":0.25, \"pc_max\":2.0,\n",
      "       \"pd_min\":2.0, \"pd_max\":16.0}\n",
      "\n",
      "def rebin_psd(freq, ps, n=10, type='average'):\n",
      "\n",
      "    nbins = int(len(freq)/n)\n",
      "    df = freq[1] - freq[0]\n",
      "    T = freq[-1] - freq[0] + df\n",
      "    bin_df = df*n\n",
      "    binfreq = np.arange(nbins)*bin_df + bin_df/2.0 + freq[0]\n",
      "\n",
      "    #print(\"len(ps): \" + str(len(ps)))\n",
      "    #print(\"n: \" + str(n))\n",
      "    \n",
      "    nbins_new = int(len(ps)/n)\n",
      "    ps_new = ps[:nbins_new*n]\n",
      "    binps = np.reshape(np.array(ps_new), (nbins_new, n))\n",
      "    binps = np.sum(binps, axis=1)\n",
      "    if type in [\"average\", \"mean\"]:\n",
      "        binps = binps/np.float(n)\n",
      "    else:\n",
      "        binps = binps\n",
      "\n",
      "    if len(binfreq) < len(binps):\n",
      "        binps= binps[:len(binfreq)]\n",
      "\n",
      "    return binfreq, binps\n",
      "\n",
      "def timeseries_features(seg):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    fmean = np.mean(counts)\n",
      "    fmedian = np.median(counts)\n",
      "    fvar = np.var(counts)\n",
      "    return fmean, fmedian, fvar\n",
      "\n",
      "\n",
      "def psd_features(seg, pcb):\n",
      "    \"\"\"\n",
      "    Computer PSD-based features.\n",
      "    seg: data slice of type [times, count rates, count rate error]^T\n",
      "    pcb: frequency bands to use for power colours\n",
      "    \"\"\"\n",
      "    \n",
      "    times = seg[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "\n",
      "    counts = seg[:,1]*dt\n",
      "    #ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "    freq, ps = make_psd(seg, navg=1)\n",
      "    #print(\"len(ps), before: \" + str(len(ps)))\n",
      "    if times[-1]-times[0] >= 2.*256.:\n",
      "        tlen = (times[-1]-times[0])\n",
      "        nrebin = np.round(tlen/256.)\n",
      "        freq, ps = rebin_psd(freq, ps, n=nrebin, type='average')\n",
      "    \n",
      "    #print(\"len(ps), after: \" + str(len(ps)))\n",
      "\n",
      "    freq = np.array(freq[1:])\n",
      "    ps = ps[1:]\n",
      "\n",
      "    fmax_ind = np.where(ps == np.max(ps))[0]\n",
      "    maxfreq = freq[fmax_ind[0]]\n",
      "\n",
      "    ## find power in spectral bands for power-colours\n",
      "    pa_min_freq = freq.searchsorted(pcb[\"pa_min\"])\n",
      "    pa_max_freq = freq.searchsorted(pcb[\"pa_max\"])\n",
      "\n",
      "    pb_min_freq = freq.searchsorted(pcb[\"pb_min\"])\n",
      "    pb_max_freq = freq.searchsorted(pcb[\"pb_max\"])\n",
      "\n",
      "    pc_min_freq = freq.searchsorted(pcb[\"pc_min\"])\n",
      "    pc_max_freq = freq.searchsorted(pcb[\"pc_max\"])\n",
      "\n",
      "    pd_min_freq = freq.searchsorted(pcb[\"pd_min\"])\n",
      "    pd_max_freq = freq.searchsorted(pcb[\"pd_max\"])\n",
      "\n",
      "    psd_a = np.sum(ps[pa_min_freq:pa_max_freq])\n",
      "    psd_b = np.sum(ps[pb_min_freq:pb_max_freq])\n",
      "    psd_c = np.sum(ps[pc_min_freq:pc_max_freq])\n",
      "    psd_d = np.sum(ps[pd_min_freq:pd_max_freq])\n",
      "    pc1 = np.sum(ps[pc_min_freq:pc_max_freq])/np.sum(ps[pa_min_freq:pa_max_freq])\n",
      "    pc2 = np.sum(ps[pb_min_freq:pb_max_freq])/np.sum(ps[pd_min_freq:pd_max_freq])\n",
      "    \n",
      "    return maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2\n",
      "\n",
      "\n",
      "def total_psd(seg, bins):\n",
      "    times = seg[:,0] \n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "    counts = seg[:,1]*dt\n",
      "    \n",
      "    ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "    binfreq = np.logspace(np.log10(ps.freq[1]), np.log10(ps.freq[-1]), bins)\n",
      "    binps, bin_edges, binno = scipy.stats.binned_statistic(ps.freq[1:], ps.ps[1:], statistic=\"mean\", bins=binfreq)\n",
      "    return binps\n",
      "    \n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_psd(segment, navg=1):\n",
      "    \n",
      "    times = segment[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "\n",
      "    counts = segment[:,1]*dt\n",
      "    \n",
      "    tseg = times[-1]-times[0]\n",
      "    nlc = len(times)\n",
      "    nseg = int(nlc/navg) \n",
      "    \n",
      "    if navg == 1:\n",
      "        ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "        return ps.freq, ps.ps\n",
      "    else:\n",
      "        ps_all = []\n",
      "        for n in xrange(navg):\n",
      "            t_small = times[n*nseg:(n+1)*nseg]\n",
      "            c_small = counts[n*nseg:(n+1)*nseg]\n",
      "            ps = powerspectrum.PowerSpectrum(t_small, counts=c_small, norm=\"rms\")\n",
      "            ps_all.append(ps.ps)\n",
      "        \n",
      "        #print(np.array(ps_all).shape) \n",
      "    \n",
      "        ps_all = np.average(np.array(ps_all), axis=0)\n",
      "\n",
      "        #print(ps_all.shape) \n",
      "    \n",
      "        return ps.freq, ps_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we'll write functions for hardness ratio based features.\n",
      "Note that the limits for the hardness ratio are derived from the *labelled data only*! We might need to check this again when we consider the whole sample!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import astroML.stats\n",
      "\n",
      "#hr_limits = [[0.292, 0.820],[0.046, 0.708]]\n",
      "#hid_limits = [[0.1, 1.5], [0.0, 6000.0]]\n",
      "\n",
      "\n",
      "def compute_hrlimits(hr1, hr2):\n",
      "    min_hr1 = np.min(hr1)\n",
      "    max_hr1 = np.max(hr1)\n",
      "\n",
      "    min_hr2 = np.min(hr2)\n",
      "    max_hr2 = np.max(hr2)\n",
      "    return [[min_hr1, max_hr1], [min_hr2, max_hr2]]\n",
      "\n",
      "\n",
      "def hr_maps(seg, bins=30):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    \n",
      "    hr_limits = compute_hrlimits(hr1, hr2)\n",
      "\n",
      "    h, xedges, yedges = np.histogram2d(hr1, hr2, bins=bins, \n",
      "                                       range=hr_limits)\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "    \n",
      "    return xedges, yedges, h\n",
      "\n",
      "def hr_fitting(seg):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "\n",
      "    # compute the robust statistics\n",
      "    #(mu_r, sigma1_r,\n",
      "    # sigma2_r, alpha_r) = astroML.stats.fit_bivariate_normal(hr1, hr2, robust=True)\n",
      "    #if any(np.isnan(mu_r)) or np.isnan(sigma1_r) or np.isnan(sigma2_r) or np.isnan(alpha_r):\n",
      "    #    print(\"mu_r: \" + str(mu_r))\n",
      "    #    print(\"sigma1_r: \" + str(sigma1_r))\n",
      "    #    print(\"sigma2_r: \" + str(sigma2_r))\n",
      "    #    print(\"alpha_r: \" + str(alpha_r))\n",
      " \n",
      "    mu1 = np.mean(hr1)\n",
      "    mu2 = np.mean(hr2)\n",
      "    cov = np.cov(hr1, hr2)\n",
      "    return mu1, mu2, cov.flatten()\n",
      "#    return mu_r, sigma1_r, sigma2_r, alpha_r\n",
      "    \n",
      "def hid_maps(seg, bins=30):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr2 = high_counts/low_counts\n",
      "    \n",
      "    hid_limits = compute_hrlimits(hr2, counts)\n",
      "    \n",
      "    h, xedges, yedges = np.histogram2d(hr2, counts, bins=bins, \n",
      "                                       range=hid_limits)\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "    \n",
      "    return xedges, yedges, h\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lcshape_features(seg, dt=1.0):\n",
      "    \n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "\n",
      "    dt_small = times[1:]-times[:-1]\n",
      "    dt_small = np.min(dt_small)\n",
      "\n",
      "    nbins = np.round(dt/dt_small)\n",
      "\n",
      "    bintimes, bincounts = rebin_psd(times, counts, nbins)\n",
      "    \n",
      "    return bincounts\n",
      "\n",
      "def extract_lc(seg):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    return [times, counts, hr1, hr2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use this code to extract some features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_features(seg, bins=30, navg=4, lc=True, hr=True):\n",
      "    features = []\n",
      "    if lc:\n",
      "        lc_all = []\n",
      "    if hr:\n",
      "        hr_all = []\n",
      "    for s in seg:\n",
      "        features_temp = []\n",
      "        fmean, fmedian, fvar = timeseries_features(s)\n",
      "        features_temp.extend([fmean, fmedian, fvar])\n",
      "\n",
      "        maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2 = psd_features(s, pcb)\n",
      "        \n",
      "        \n",
      "        features_temp.extend([maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2])\n",
      "        \n",
      "        #whole_ps = total_psd(s,navg=navg)\n",
      "        #features_temp.extend(whole_ps[1:])\n",
      "\n",
      "        #lc = lcshape_features(s, dt=1.0)\n",
      "        #features_temp.extend(lc)\n",
      "        \n",
      "        #xedges, yedges, h = hr_maps(s, hr_limits, bins=bins)\n",
      "        #features_temp.extend(h.flatten())\n",
      "        mu1, mu2, cov = hr_fitting(s)\n",
      "        features_temp.extend([mu1, mu2])\n",
      "        features_temp.extend(cov)\n",
      "        \n",
      "        features.append(features_temp)\n",
      "        \n",
      "        if lc or hr:\n",
      "            lc_temp = extract_lc(s)\n",
      "        if lc:\n",
      "            #print(\"appending light curve\")\n",
      "            lc_all.append([lc_temp[0], lc_temp[1]])\n",
      "        if hr:\n",
      "            #print(\"appending hardness ratios\")\n",
      "            hr_all.append([lc_temp[2], lc_temp[3]])\n",
      "            \n",
      "    print(\"I am about to make a dictionary\")\n",
      "    fdict = {\"features\": features}\n",
      "    print(fdict.keys)\n",
      "    if lc:\n",
      "        print(\"I am in lc\")\n",
      "        #features.append(lc_all)\n",
      "        fdict[\"lc\"] = lc_all\n",
      "    if hr:\n",
      "        print(\"I am in hr\")\n",
      "        #features.append(hr_all)\n",
      "        fdict[\"hr\"] = hr_all\n",
      "    print(fdict.keys())\n",
      "    return fdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train = make_features(seg_train, bins=30, navg=4)\n",
      "features_val = make_features(seg_val, bins=30, navg=4)\n",
      "features_test = make_features(seg_test, bins=30, navg=4)\n",
      "\n",
      "#print(\"features_train: \" + str(features_train.shape))\n",
      "#print(\"features_val: \" + str(features_val.shape))\n",
      "#print(\"features_test: \" + str(features_test.shape))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I am about to make a dictionary\n",
        "<built-in method keys of dict object at 0x1163936e0>\n",
        "I am in lc\n",
        "I am in hr\n",
        "['hr', 'features', 'lc']\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x116393a28>\n",
        "I am in lc\n",
        "I am in hr\n",
        "['hr', 'features', 'lc']\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x116393d70>\n",
        "I am in lc\n",
        "I am in hr\n",
        "['hr', 'features', 'lc']\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "['hr', 'features', 'lc']"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's save our features in a pickle object for easy extraction in the next step. We're going to make a dictionary with keywords \"train\", \"test\", \"val\", each of which has a training vector and the corresponding labels. Rows are samples, columns features. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labelled_features = {\"train\": [features_train, labels_train],\n",
      "                     \"val\": [features_val, labels_val],\n",
      "                     \"test\": [features_test, labels_test]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_labelled_features_30bins.dat\", \"w\")\n",
      "pickle.dump(labelled_features, f)\n",
      "f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "for i,f in enumerate(features_train[\"features\"]):\n",
      "\n",
      "    if any(np.isnan(f)):\n",
      "        print(\"NaN in sample row %i\"%i)\n",
      "    if any(np.isinf(f)):\n",
      "        print(\"inf sample row %i\"%i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feature Extraction for both labelled and unlabelled data\n",
      "\n",
      "Because I have so many big data files, I need to be a bit careful with the feature extraction for *all* data. Also, at least for the supervised classification, I want to separate labelled and unlabelled data for training/validation/testing purposes. This is what I'll attempt below. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## first, make a list of light curves\n",
      "files = glob.glob(\"LC_*combined.dat\")\n",
      "print(\"There are %i observation files.\"%len(files))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 1706 observation files.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we define a helper function that goes through each observation, finds data gaps and splits the data along these gaps into uneven segments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import generaltools as gt\n",
      "\n",
      "## split light curves into segments without gaps\n",
      "def remove_gaps(d, state):\n",
      "        \n",
      "    dt_data = d[1:,0]-d[:-1,0]\n",
      "    dt_min = np.min(dt_data)\n",
      "    print(\"dt_min: \" + str(dt_min))\n",
      "    tol = dt_min*0.01\n",
      "    \n",
      "    ### split data with breaks\n",
      "    breaks = np.where(dt_data > dt_min+tol)[0]\n",
      "    #print(breaks)\n",
      "    d_all = []\n",
      "    \n",
      "    if len(breaks) == 0:\n",
      "        dtemp = d\n",
      "        d_all.append([dtemp, state])\n",
      "    else:\n",
      "        for i,b in enumerate(breaks):\n",
      "            #print(\"Break in light curve at time bin %i; length of break dt = %.3f\"%(b, dt_data[b]))\n",
      "            if i == 0:\n",
      "                if b == 0:\n",
      "                    #print(\"First break is at first time bin\")\n",
      "                    continue\n",
      "                else:\n",
      "                    #print(\"I am extracting the data before the break\")\n",
      "                    dtemp = d[:b]\n",
      "\n",
      "            else:\n",
      "                #print(\"I am extracting data after break \" + str(i))\n",
      "                dtemp = d[breaks[i-1]+1:b]\n",
      "\n",
      "            d_all.append([dtemp, state])\n",
      "\n",
      "        ## last segment\n",
      "        #print(\"I am computing last segment of file\")\n",
      "        dtemp = d[b+1:]\n",
      "        d_all.append([dtemp, state])\n",
      "\n",
      "    return d_all\n",
      "\n",
      "## helper function that bins light curves\n",
      "def bin_lightcurve(dtemp, nbins):\n",
      "    tbinned_times, tbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,1], n=nbins, type=\"average\")\n",
      "    #print(\"dt: \" + str(tbinned_times[1]-tbinned_times[0]))\n",
      "    lbinned_times, lbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,2], n=nbins, type=\"average\")\n",
      "    hbinned_times, hbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,3], n=nbins, type=\"average\")\n",
      "    dshort = np.transpose(np.array([tbinned_times, tbinned_counts, lbinned_counts, hbinned_counts]))\n",
      "    return dshort\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below is the function that actually does all the feature extraction. Note that for extracting different features, you'll need to change those in the function below. I might make that more robust in the future."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import convert_belloni \n",
      "\n",
      "def make_features_all(files, subset=\"all\", labels=None, bin_data=True, bin_dt=0.125,\n",
      "                      seg_length=256., overlap=64., bins=30, navg=4):\n",
      "    \"\"\"\n",
      "    Extract features from light curves in files.\n",
      "    subset can be \"all\", \"labelled\", \"unlabelled\" to extract either\n",
      "    the entire set in files, those with labels or those without.\n",
      "    \"\"\"\n",
      "    \n",
      "    ## for labelled data, extract list with labels\n",
      "    #belloni_states = convert_belloni.main()\n",
      "    #belloni_turned = turn_states(belloni_states)\n",
      "    belloni_turned = convert_belloni.convert_belloni_clean()\n",
      "\n",
      "    ## lists for the features and labels\n",
      "    features_all = []\n",
      "    labels_all = []\n",
      "    \n",
      "    ## loop over all files\n",
      "    for i,f in enumerate(files):\n",
      "        #print(\"I am on file %i with filename %s \"%(i,f))\n",
      "\n",
      "        ## figure out whether a state exists for this observation\n",
      "        fstring = f.split(\"_\")[1]\n",
      "        if fstring in belloni_turned:\n",
      "            state = belloni_turned[fstring]\n",
      "            if subset == \"unlabelled\":\n",
      "                continue\n",
      "        else:\n",
      "            state = None\n",
      "            if subset == \"labelled\":\n",
      "                continue\n",
      "        \n",
      "        ## load data\n",
      "        d = np.loadtxt(f)\n",
      "        \n",
      "        ## remove gaps from observation, makes a list of \n",
      "        ## uneven segments split along data gaps\n",
      "        d_all = remove_gaps(d, state)\n",
      "        \n",
      "        ## loop over all segments to extract features\n",
      "        features_temp, labels_temp = [], []\n",
      "        \n",
      "        ## if bin_data is True, bin light curves to \n",
      "        ## resolution specified in bin_dt\n",
      "        if bin_data:\n",
      "            d_all_bin = []\n",
      "            for da in d_all:\n",
      "                #print(\"len(da[0]): \" + str(len(da[0])))\n",
      "                if len(da[0]) < 2:\n",
      "                    continue\n",
      "                dt_data = da[0][1:,0]-da[0][:-1,0]\n",
      "                dt_data = np.min(dt_data)\n",
      "                nbins = np.round(bin_dt/dt_data)\n",
      "                dtemp = bin_lightcurve(da[0], nbins)\n",
      "                d_all_bin.append([dtemp, da[1]])\n",
      "            \n",
      "            d_all = d_all_bin\n",
      "        \n",
      "        \n",
      "        ## next, extract even-length segments\n",
      "        seg, labels = extract_segments(d_all, seg_length, overlap)\n",
      "\n",
      "        labels_all.extend(labels)\n",
      "        features_temp = make_features(seg, bins=30, navg=4)\n",
      "        #print(\"Feature shape: \" + str(features_temp.shape))\n",
      "        features_all.extend(list(features_temp))\n",
      "    \n",
      "    return np.array(features_all), labels_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's extract all the unlabelled features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_unlabelled, l_unlabelled = make_features_all(files, subset=\"unlabelled\", labels=None, bin_data=True, bin_dt=0.125,\n",
      "                      seg_length=256., overlap=64., bins=30, navg=4)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'files' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-17-e7e1d8405d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m f_unlabelled, l_unlabelled = make_features_all(files, subset=\"unlabelled\", labels=None, bin_data=True, bin_dt=0.125,\n\u001b[0m\u001b[1;32m      2\u001b[0m                       seg_length=256., overlap=64., bins=30, navg=4)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_unlabelled_features.dat\", \"w\")\n",
      "pickle.dump([f_unlabelled, l_unlabelled], f)\n",
      "f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's do the same on labelled data. This is a bit more complex, because we need to distinguish training, validation and test data sets. We'll do this by splitting up the file names into batches, and then process them separately."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## shuffle list of light curves\n",
      "np.random.shuffle(files)\n",
      "nfiles = len(files)\n",
      "\n",
      "train_frac = 0.5\n",
      "validation_frac = 0.25\n",
      "test_frac = 0.25\n",
      "\n",
      "## let's pull out light curves for three data sets into different variables.\n",
      "files_train = files[:int(train_frac*nfiles)]\n",
      "files_val = files[int(train_frac*nfiles):int((train_frac + validation_frac)*nfiles)]\n",
      "files_test = files[int((train_frac + validation_frac)*nfiles):]\n",
      "\n",
      "print(\"len(train): \" + str(len(files_train)))\n",
      "print(\"len(val): \" + str(len(files_val)))\n",
      "print(\"len(test): \" + str(len(files_test)))\n",
      "\n",
      "f_train, l_train = make_features_all(files_train, subset=\"labelled\", labels=None, bin_data=True, bin_dt=0.125,\n",
      "                      seg_length=256., overlap=64., bins=30, navg=4)\n",
      "f_val, l_val = make_features_all(files_val, subset=\"labelled\", labels=None, bin_data=True, bin_dt=0.125,\n",
      "                      seg_length=256., overlap=64., bins=30, navg=4)\n",
      "f_test, l_test = make_features_all(files_test, subset=\"labelled\", labels=None, bin_data=True, bin_dt=0.125,\n",
      "                      seg_length=256., overlap=64., bins=30, navg=4)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'files' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-3f1a63734bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## shuffle list of light curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_frac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"training set: \" + str(f_train.shape))\n",
      "print(\"validation set: \" + str(f_va.shape))\n",
      "print(\"test set: \" + str(f_test.shape))\n",
      "\n",
      "labelled_features = {\"train\": [f_train, l_train],\n",
      "                     \"val\": [f_va, l_val],\n",
      "                     \"test\": [f_test, l_test]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "training set: (4194, 910)\n",
        "validation set: (2534, 910)\n",
        "test set: (2002, 910)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_labelled_features.dat\", \"w\")\n",
      "pickle.dump(labelled_features, f)\n",
      "f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using the Script\n",
      "\n",
      "For the actual analysis, I have put the stuff above into a script `feature_extraction.py`. We will run it on segments with duration between 512 seconds and 2048 seconds, with a 128 second overlap, with a 60% training set and 20% validation and test set each. The number of bins for the histogram is 20 by 20, for a total of 400 features. The periodogram will be averaged between 30 consecutive segments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feature_extraction\n",
      "datadir = \"../\"\n",
      "feature_extraction.extract_all(d_all, datadir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1024 segments, summary\n",
        "Number of light curves:  426\n",
        "There are 213 light curves in the training set.\n",
        "There are 106 light curves in the validation set.\n",
        "There are 107 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      31\n",
        "delta     23\n",
        "chi4      20\n",
        "chi1      20\n",
        "rho       15\n",
        "mu        15\n",
        "nu        14\n",
        "gamma     13\n",
        "beta      12\n",
        "theta     11\n",
        "phi       10\n",
        "alpha      9\n",
        "lambda     8\n",
        "chi3       7\n",
        "kappa      5\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      24\n",
        "gamma     14\n",
        "chi4      13\n",
        "beta       9\n",
        "mu         7\n",
        "chi1       7\n",
        "phi        7\n",
        "theta      6\n",
        "nu         5\n",
        "delta      5\n",
        "chi3       4\n",
        "lambda     3\n",
        "alpha      1\n",
        "rho        1\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi4      14\n",
        "chi2      13\n",
        "delta     12\n",
        "alpha      9\n",
        "nu         8\n",
        "rho        8\n",
        "beta       7\n",
        "phi        7\n",
        "lambda     6\n",
        "gamma      5\n",
        "theta      5\n",
        "mu         5\n",
        "chi3       4\n",
        "chi1       3\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x11fc86d70>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x11fc86398>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the training set ...\n",
        "Checking for NaN in the test set ...\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x11fc8f280>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the validation set ...\n",
        "1024 segments, hr full"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of light curves:  426\n",
        "There are 213 light curves in the training set.\n",
        "There are 106 light curves in the validation set.\n",
        "There are 107 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      28\n",
        "delta     22\n",
        "gamma     19\n",
        "chi4      18\n",
        "chi1      17\n",
        "beta      16\n",
        "phi       16\n",
        "rho       15\n",
        "mu        11\n",
        "chi3      11\n",
        "nu        10\n",
        "theta     10\n",
        "lambda     8\n",
        "alpha      8\n",
        "kappa      4\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      22\n",
        "chi4      12\n",
        "delta     10\n",
        "mu         9\n",
        "beta       8\n",
        "lambda     7\n",
        "nu         7\n",
        "theta      6\n",
        "chi1       6\n",
        "alpha      5\n",
        "gamma      4\n",
        "rho        4\n",
        "phi        3\n",
        "chi3       2\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      18\n",
        "chi4      17\n",
        "nu        10\n",
        "gamma      9\n",
        "delta      8\n",
        "mu         7\n",
        "chi1       7\n",
        "theta      6\n",
        "alpha      6\n",
        "rho        5\n",
        "phi        5\n",
        "beta       4\n",
        "lambda     2\n",
        "chi3       2\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10c6eea28>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10cc856e0>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the training set ...\n",
        "Checking for NaN in the test set ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10ccf84b0>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the validation set ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1024 segments, ps full"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of light curves:  426\n",
        "There are 213 light curves in the training set.\n",
        "There are 106 light curves in the validation set.\n",
        "There are 107 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      27\n",
        "chi4      26\n",
        "delta     20\n",
        "gamma     17\n",
        "beta      17\n",
        "rho       14\n",
        "mu        14\n",
        "theta     13\n",
        "alpha     13\n",
        "chi1      13\n",
        "phi       12\n",
        "nu        10\n",
        "lambda     8\n",
        "chi3       5\n",
        "kappa      4\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      21\n",
        "chi4      12\n",
        "delta     11\n",
        "lambda     9\n",
        "gamma      8\n",
        "phi        8\n",
        "nu         7\n",
        "rho        6\n",
        "mu         6\n",
        "beta       5\n",
        "chi1       4\n",
        "chi3       4\n",
        "theta      3\n",
        "alpha      1\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2     20\n",
        "chi1     13\n",
        "nu       10\n",
        "delta     9\n",
        "chi4      9\n",
        "gamma     7\n",
        "mu        7\n",
        "theta     6\n",
        "beta      6\n",
        "chi3      6\n",
        "alpha     5\n",
        "rho       4\n",
        "phi       4\n",
        "kappa     1\n",
        "dtype: int64\n",
        "================================================================\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x11fc86398>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10cd48280>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the training set ...\n",
        "Checking for NaN in the test set ...\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x11fc867f8>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the validation set ...\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(feature_extraction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "<module 'feature_extraction' from 'feature_extraction.py'>"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "============================================================\n",
      "## Low-Memory Feature Extraction\n",
      "\n",
      "Some code I might need in the future when I'm doing this on thousands and thousands\n",
      "of light curves, so it's here for future reference and completion. \n",
      "This code includes both parts of the data extraction and the feature extraction, so it'll\n",
      "probably need to be heavily modified by the time I get around to doing this on the full \n",
      "data set, but it's here as a start.\n",
      "\n",
      "Keeping thousands of light curves in memory quickly drains resources. \n",
      "\n",
      "Below is an implementation of the code reading in light curves and extracting\n",
      "features that's less computationally expensive by only keeping the resulting \n",
      "features in memory (light curves can be retained by setting keep_lcs = True)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features_lowmem(datafiles, pcb, bin_data=True, bin_res = 0.125, tseg=256., overlap_time=64., \\\n",
      "                            label_only=True, labels=\"clean\", hr=True, output_lc=True):\n",
      "    \"\"\"\n",
      "    Extract Observing Mode data from a list of text files with columns \n",
      "    #times \\t total count rate \\t low energy count rate \\t high energy count rate\n",
      "    \n",
      "    if bin_data=True, the data will be rebinned to the resolution specified in bin_res.\n",
      "    If label_only is True, only data with (manually determined) labels will be extracted;\n",
      "    either from the whole set of observations used in Belloni+ 2000 (labels=\"all\") or from\n",
      "    the cleaned set with only those observations where the state does not change during the \n",
      "    observation (labels=\"clean\")\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    if label_only:\n",
      "        if labels == \"clean\":\n",
      "            belloni_states = convert_belloni.main()\n",
      "            belloni_turned = turn_states(belloni_states)\n",
      "        else:\n",
      "            belloni_turned = convert_belloni_clean()\n",
      "    else:\n",
      "        belloni_turned = [None]\n",
      "    \n",
      "    all_features = {\"fmu\":[], \"fvar\":[], \"labels\":[], \"fmax\":[], \n",
      "                    \"psd_a\":[], \"psd_b\":[], \"psd_c\":[], \"psd_d\":[],\n",
      "                    \"pc1\":[], \"pc2\":[]}\n",
      "    if output_lc:\n",
      "        all_features[\"lc_all\"] = []\n",
      "    if hr:\n",
      "        all_features[\"hr1\"] = []\n",
      "        all_features[\"hr2\"] = []\n",
      "        all_features[\"hr1 mean\"] = []\n",
      "        all_features[\"hr2 mean\"] =[]\n",
      "        all_features[\"hr1 var\"] = []\n",
      "        all_features[\"hr2 var\"] = []\n",
      "    for f in datafiles:\n",
      "        print(\"I am on file \" + str(f))\n",
      "        fstring = f.split(\"_\")[1]\n",
      "        if fstring in belloni_turned:\n",
      "            state = belloni_turned[fstring]\n",
      "        else:\n",
      "            state = None\n",
      "            if label_only:\n",
      "                print(\"No class assigned to file %s. Skipping ...\"%f)\n",
      "                continue\n",
      "                \n",
      "        d = np.loadtxt(f)\n",
      "        #times = d[:,0]\n",
      "        #counts = d[:,1]\n",
      "        #plt.plot(times, counts)\n",
      "        dt_data = d[1:,0]-d[:-1,0]\n",
      "\n",
      "        dt_min = np.min(dt_data)\n",
      "        \n",
      "        #print(\"dt_min: \" + str(dt_min))\n",
      "        #nseg = int(tseg/dt_min) ## number of bins per segment\n",
      "        #noverlap = int(overlap_time/dt_min)\n",
      "        #print(\"nseg: \" + str(nseg))\n",
      "        #print(\"noverlap: \" + str(noverlap))\n",
      "\n",
      "        nbins = int(bin_res/dt_min)\n",
      "        #print(\"bin_res: \" + str(bin_res))\n",
      "        #print(\"dt_min: \" + str(dt_min))\n",
      "        #print(\"nbins: \" + str(nbins))\n",
      "\n",
      "        ### split data with breaks\n",
      "        breaks = np.where(dt_data > 0.008)[0]\n",
      "        #print(breaks)\n",
      "        if len(breaks) == 0:\n",
      "            dtemp = d\n",
      "            if bin_data:\n",
      "                #nbins = int(bin_res/tres)\n",
      "                dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "            else:\n",
      "                dshort = dtemp  \n",
      "            features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                        clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "            for k in all_features.keys():\n",
      "                all_features[k].append(features[k])\n",
      "\n",
      "\n",
      "        else:\n",
      "            for i,b in enumerate(breaks):\n",
      "                #print(\"Break in light curve at time bin %i; length of break dt = %.3f\"%(b, dt_data[b]))\n",
      "                if i == 0:\n",
      "                    if b == 0:\n",
      "                        #print(\"First break is at first time bin\")\n",
      "                        continue\n",
      "                    else:\n",
      "                        #print(\"I am extracting the data before the break\")\n",
      "                        dtemp = d[:b]\n",
      "                        if bin_data:\n",
      "                            #nbins = int(bin_res/tres)\n",
      "                            dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "                        else:\n",
      "                            dshort = dtemp\n",
      "                            \n",
      "                        print(\"dshort.shape: \" + str(dshort.shape))\n",
      "\n",
      "                        features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                    clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "                        \n",
      "                        for k in all_features.keys():\n",
      "                            all_features[k].append(features[k])\n",
      "                \n",
      "                else:\n",
      "                    #print(\"I am extracting data after break \" + str(i))\n",
      "                    dtemp = d[breaks[i-1]+1:b]\n",
      "                    if bin_data:\n",
      "                        #nbins = int(bin_res/tres)\n",
      "                        dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "                    else:\n",
      "                        dshort = dtemp\n",
      "                    print(\"dshort.shape: \" + str(dshort.shape))\n",
      "\n",
      "                    features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                                clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "                    for k in all_features.keys():\n",
      "                        all_features[k].append(features[k])\n",
      "\n",
      "\n",
      "\n",
      "            ## last segment\n",
      "            #print(\"I am computing last segment of file\")\n",
      "            dtemp = d[b+1:]\n",
      "            if bin_data:\n",
      "                #nbins = int(bin_res/tres)\n",
      "                dshort = bin_lightcurve(dtemp, nbins)                    \n",
      "            else:\n",
      "                dshort = dtemp\n",
      "            print(\"dshort.shape: \" + str(dshort.shape))\n",
      "            features = extract_features([[dshort, state]], pcb, tseg=tseg, overlap_time=overlap_time,\n",
      "                                        clean_only=label_only, hr=hr, output_lc=output_lc)\n",
      "            for k in all_features.keys():\n",
      "                all_features[k].append(features[k])\n",
      "\n",
      "    return all_features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}