{
 "metadata": {
  "name": "",
  "signature": "sha256:76acca0aa824c826bac282d615bb2091c66a8654aa5c5560b617a9c40ddb1c99"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Extraction\n",
      "\n",
      "Our data comes in the form of observations done in X-rays over a course of 16 years.\n",
      "Each observation is of the order of few thousand seconds long (sometimes less, rarely more) of \n",
      "uninterrupted data. For each unterrupted time interval in which data was taken, we have **time series**\n",
      "with a 0.125 second time resolution, taken in four different **energy bands**. \n",
      "Additionally, in each observation, the source could have been in one state continuously, but it also \n",
      "could have changed state in the middle. GRS 1915+105 is known to switch between two states several times\n",
      "on occasion before settling into a new state. \n",
      "\n",
      "In summary, our data consists of four simultaneous (correlated, but not necessarily identical) time series.\n",
      "Our task is to extract meaningful **features** for these time series that will capture the behaviour of \n",
      "the source and the different states it goes through such that they will be recognizable by a machine learning\n",
      "algorithm. \n",
      "\n",
      "With the exception of a few deep learning methods (which we will not touch here!), we cannot simply put our \n",
      "data into a classifier directly, for several reasons:\n",
      "- in order to capture the behaviour seen by human classifiers accurately, we need to feed long light curves into the classifier; for 1024s time series, that translates to 1024x4x4 samples, at which point dimensionality is a definite problem: *we need to extract features that will summarize the behaviour in as few numbers as possible*\n",
      "- even if our time series are longer than the typical behaviour of the source, machine learning classifiers are not phase invariant: if I have two time series of a perfectly sinusoidal signal that are out of phase with each other, the classifier will consider them drastically different, because it compares sample by samples (time bin by time bin): *this means we need to extract features that capture the behaviour of the time series in a phase-invariant way*\n",
      "- there is behaviour in the time series that may be better captures by an alternative representation: *in particular, quasi-periodic features may be better captured in the Fourier domain than in the time domain*\n",
      "- GRS1915+105 exhibits clear energy-dependent behaviour over time; that is, the energy spectrum of the source changes both within states and between states. *We need to extract features that describe this energy-dependent behaviour*\n",
      "\n",
      "### A set of features to explore\n",
      "\n",
      "We will extract features from three domains and will later explore how predictive they are on supervised machine learning tasks. Features will summarize properties of the four simultaneous time series in three domains: the time domain, the Fourier domain, and the energy domain.\n",
      "\n",
      "Time domain features are:\n",
      "- mean count rate `fmu`\n",
      "- variance in the total light curve `fvar`\n",
      "\n",
      "Fourier domain features are:\n",
      "- integrated power in 4 bands: \n",
      "    - PA: 0.0039-0.031 Hz, \n",
      "    - PB: 0.031-0.25 Hz, \n",
      "    - PC: 0.25-2.0 Hz and \n",
      "    - PD: 2.0-16.0 Hz\n",
      "- power colours: PC/PA and PB/PD\n",
      "- frequency at which maximum power is observed\n",
      "\n",
      "\n",
      "Energy domain features are:\n",
      "- mean and covariance of hardness ratios in 2 bands (processed data only, NOT Standard 1 data)\n",
      "    - (2-6keV)/(2-13keV)\n",
      "    - (9-20kev)/(2-13keV) \n",
      "\n",
      "\n",
      "\n",
      "First, we'll need to load the data. This assumes that you've saved a pickle file with a list of $N$ light curves and states, `[[lc1, state1], [lc2, state2], ..., [lc_n, state_n]]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import glob\n",
      "import scipy.stats\n",
      "\n",
      "\n",
      "import cPickle as pickle\n",
      "import pandas as pd\n",
      "import sys\n",
      "\n",
      "import powerspectrum\n",
      "\n",
      "sys.path.append(\"/Users/danielahuppenkothen/work/repositories/LightEcho/code/\")\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import linearfilter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"../../grs1915_all_125ms.dat\")\n",
      "d_all = pickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break down the frequencies of states in these light curves. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Number of light curves:  \" + str(len(d_all)))\n",
      "states = [d[1] for d in d_all]\n",
      "st = pd.Series(states)\n",
      "st.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of light curves:  2829\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "chi2      68\n",
        "chi4      47\n",
        "delta     40\n",
        "gamma     32\n",
        "chi1      30\n",
        "beta      28\n",
        "nu        27\n",
        "mu        27\n",
        "rho       24\n",
        "phi       24\n",
        "theta     22\n",
        "alpha     19\n",
        "lambda    17\n",
        "chi3      15\n",
        "kappa      6\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_labelled = [d for d in d_all if d[1] != None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"There are a total number of %i light curves.\"%len(d_all))\n",
      "print(\"%i of them are labelled. \"%len(d_labelled))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are a total number of 2829 light curves.\n",
        "426 of them are labelled. \n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import seaborn as sns\n",
      "datadir=\"./\"\n",
      "\n",
      "## number of light curves to plot\n",
      "## just so that I don't plot *all* of them \n",
      "## every time I run this cell\n",
      "nplot = 1\n",
      "for i,d in enumerate(d_labelled[:nplot]):\n",
      "#d = d_labelled[0]\n",
      "    plt.figure(figsize=(16,7))\n",
      "    plt.plot(d[0][:,0], d[0][:,1])\n",
      "    plt.xlim(d[0][0,0], d[0][-1,0])\n",
      "    plt.title(\"State %s\"%d[1], fontsize=24)\n",
      "    plt.savefig(datadir+\"obs%i_lc.pdf\"%i, format=\"pdf\")\n",
      "    plt.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that the classes are *fairly* evenly distributed, with $\\chi_2$ having the most light curves in their samples, and $\\kappa$ being undersampled. \n",
      "\n",
      "In the next step, we'll pick out training set, validation set and test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## total number of light curves\n",
      "n_lcs = len(d_all)\n",
      "\n",
      "## Set the seed to I will always pick out the same light curves.\n",
      "np.random.seed(20150608)\n",
      "\n",
      "## shuffle list of light curves\n",
      "np.random.shuffle(d_all)\n",
      "\n",
      "train_frac = 0.6\n",
      "validation_frac = 0.2\n",
      "test_frac = 0.2\n",
      "\n",
      "## let's pull out light curves for three data sets into different variables.\n",
      "d_all_train = d_all[:int(train_frac*n_lcs)]\n",
      "d_all_val = d_all[int(train_frac*n_lcs):int((train_frac + validation_frac)*n_lcs)]\n",
      "d_all_test = d_all[int((train_frac + validation_frac)*n_lcs):]\n",
      "\n",
      "## Let's print some information about the three sets.\n",
      "print(\"There are %i light curves in the training set.\"%len(d_all_train))\n",
      "print(\"There are %i light curves in the validation set.\"%len(d_all_val))\n",
      "print(\"There are %i light curves in the test set.\"%len(d_all_test))\n",
      "for da,n in zip([d_all_train, d_all_val, d_all_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    states = [d[1] for d in da]\n",
      "    st = pd.Series(states)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 1697 light curves in the training set.\n",
        "There are 566 light curves in the validation set.\n",
        "There are 566 light curves in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      34\n",
        "chi4      33\n",
        "delta     28\n",
        "chi1      20\n",
        "gamma     19\n",
        "phi       18\n",
        "beta      14\n",
        "nu        14\n",
        "rho       12\n",
        "theta     12\n",
        "mu        12\n",
        "alpha     10\n",
        "lambda     7\n",
        "chi3       6\n",
        "kappa      4\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      15\n",
        "nu         9\n",
        "theta      8\n",
        "delta      8\n",
        "beta       8\n",
        "lambda     6\n",
        "gamma      6\n",
        "mu         6\n",
        "chi4       6\n",
        "alpha      5\n",
        "rho        4\n",
        "chi3       4\n",
        "chi1       3\n",
        "phi        2\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      19\n",
        "mu         9\n",
        "rho        8\n",
        "chi4       8\n",
        "gamma      7\n",
        "chi1       7\n",
        "beta       6\n",
        "chi3       5\n",
        "lambda     4\n",
        "nu         4\n",
        "alpha      4\n",
        "delta      4\n",
        "phi        4\n",
        "theta      2\n",
        "kappa      1\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a next step, we can make light curve segments, overlapping or not, to then pass to feature extraction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## This function is also in grs1915_utils.py!\n",
      "def extract_segments(d_all, seg_length = 256., overlap=128.):\n",
      "    \"\"\" Extract light curve segmens from a list of light curves. \n",
      "        Each element in the list is a list with two elements: \n",
      "        - an array that contains the light curve in three energy bands \n",
      "        (full, low energies, high energies) and \n",
      "        - a string containing the state of that light curve.\n",
      "        \n",
      "        The parameters are \n",
      "        - seg_length: the length of each segment. Bits of data at the end of a light curve\n",
      "        that are smaller than seg_length will not be included. \n",
      "        - overlap: This is actually the interval between start times of individual segments,\n",
      "        i.e. by default light curves start 64 seconds apart. The actual overlap is \n",
      "        seg_length-overlap\n",
      "    \"\"\"\n",
      "    segments, labels = [], [] ## labels for labelled data\n",
      "        \n",
      "    for i,d_seg in enumerate(d_all):\n",
      "        \n",
      "        ## data is an array in the first element of d_seg\n",
      "        data = d_seg[0]\n",
      "        ## state is a string in the second element of d_seg\n",
      "        state = d_seg[1]\n",
      "\n",
      "        ## compute the intervals between adjacent time bins\n",
      "        #dt_data = data[1:,0] - data[:-1,0]\n",
      "        #print(data.shape)\n",
      "        \n",
      "        ## if the light curve is shorter than the segment length,\n",
      "        ## throw out!\n",
      "        length = data[-1,0] - data[0,0]\n",
      "        if length < seg_length:\n",
      "            continue\n",
      "            \n",
      "        dt = np.min(np.diff(data[:,0]))\n",
      "        #print(\"length: \" + str(data[-1,0] - data[0,0]))\n",
      "        #print(\"dt: \" + str(dt))\n",
      "        \n",
      "        ## compute the number of time bins in a segment\n",
      "        nseg = seg_length/dt\n",
      "        ## compute the number of time bins to start of next segment\n",
      "        noverlap = overlap/dt\n",
      "        \n",
      "        istart = 0\n",
      "        iend = nseg\n",
      "        j = 0\n",
      "     \n",
      "        while iend <= len(data):\n",
      "            dtemp = data[istart:iend]\n",
      "            segments.append(dtemp)\n",
      "            labels.append(state)\n",
      "            istart += noverlap\n",
      "            iend += noverlap\n",
      "            j+=1\n",
      "        \n",
      "    return segments, labels\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll extract 1024 second long segments for the supervised classification and 256 second long segments for the unsupervised classification.\n",
      "\n",
      "For the supervised classification, we would like to capture as much of the states as observed by Belloni et al in  each light curve (such that there isn't too much variance between features of samples within a class), but we also need to retain enough light curves for a useful training set. \n",
      "\n",
      "**NOTE: DO I NEED TO WORRY ABOUT THE FACT THAT MY DATA SET IS HEAVILY UNBALANCED?**\n",
      "\n",
      "For the unsupervised classification, we are going to choose shorter segments, because in this case, we would like to model behaviour on shorter time scales, where behaviour in different states might overlap. Our hypothesis is that because we *know* that some short-time behaviour is shared between several of the human-classified classes, there is a number of micro-states we might be able to find, and use combinations of these micro-states to explain what's happening on a larger scale.\n",
      "\n",
      "In either case, we can choose some overlap between segments to augment our data set and allow the classifier to \"see\" different parts of the same light curve shape. Here, we choose 64 seconds for all further analysis!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seg_length_supervised = 1024.\n",
      "seg_length_unsupervised = 256.\n",
      "\n",
      "overlap_all = 64.\n",
      "\n",
      "## CURRENTLY RUNNING LONG SEGMENTS FOR SUPERVISED CLASSIFICATION\n",
      "seg_train, labels_train = extract_segments(d_all_train, seg_length=seg_length_supervised, overlap=overlap_all)\n",
      "seg_val, labels_val = extract_segments(d_all_val, seg_length=seg_length_unsupervised, overlap=overlap_all)\n",
      "seg_test, labels_test = extract_segments(d_all_test, seg_length=seg_length_unsupervised, overlap=overlap_all)\n",
      "\n",
      "## Let's print some details on the different segment data sets\n",
      "print(\"There are %i segments in the training set.\"%len(seg_train))\n",
      "print(\"There are %i segments in the validation set.\"%len(seg_val))\n",
      "print(\"There are %i segments in the test set.\"%len(seg_test))\n",
      "for la,n in zip([labels_train, labels_val, labels_test], [\"training\", \"validation\", \"test\"]):\n",
      "    print(\"These is the distribution of states in the %s set: \"%n)\n",
      "    st = pd.Series(la)\n",
      "    print(st.value_counts())\n",
      "    print(\"================================================================\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 20269 segments in the training set.\n",
        "There are 12270 segments in the validation set.\n",
        "There are 12498 segments in the test set.\n",
        "These is the distribution of states in the training set: \n",
        "chi2      550\n",
        "chi4      430\n",
        "chi1      412\n",
        "phi       301\n",
        "beta      259\n",
        "gamma     242\n",
        "delta     224\n",
        "rho       219\n",
        "mu        204\n",
        "theta     116\n",
        "chi3      103\n",
        "alpha      94\n",
        "nu         76\n",
        "lambda     64\n",
        "kappa      46\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the validation set: \n",
        "chi2      303\n",
        "theta     193\n",
        "lambda    177\n",
        "chi4      173\n",
        "gamma     169\n",
        "delta     151\n",
        "beta      149\n",
        "nu        144\n",
        "alpha     114\n",
        "mu        103\n",
        "rho        91\n",
        "chi3       90\n",
        "chi1       84\n",
        "kappa      49\n",
        "phi        34\n",
        "dtype: int64\n",
        "================================================================\n",
        "These is the distribution of states in the test set: \n",
        "chi2      482\n",
        "chi1      273\n",
        "mu        237\n",
        "gamma     229\n",
        "beta      192\n",
        "rho       131\n",
        "chi3      125\n",
        "phi       119\n",
        "alpha     110\n",
        "chi4      106\n",
        "nu         90\n",
        "theta      81\n",
        "delta      75\n",
        "lambda     74\n",
        "kappa      40\n",
        "dtype: int64\n",
        "================================================================\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now, we can think about actual feature extraction.\n",
      "\n",
      "Let's make some functions to extract individual features from a segment. We can then combine them at will. \n",
      "\n",
      "### Time Series Features\n",
      "\n",
      "First, we're going to extract time series features derived from the light curve:\n",
      "\n",
      "- mean, \n",
      "- median \n",
      "- variance \n",
      "- skewness\n",
      "- kurtosis\n",
      "- weights from a linear filter \n",
      "\n",
      "The linear filter requires two hyperparameters:\n",
      "1. the number of previous time bins to use in the determination of the current flux; this translates directly into the number of features added (i.e. $k=10$ will translate into 10 features added by this representation of the data)\n",
      "2. the regularization term for the ridge regression (which can also be determined via cross validation)\n",
      "\n",
      "We will determine the linear filter hyperparameters in the feature engineering step of the analysis.\n",
      "\n",
      "**NOTE: The linear filter routine takes *all* light curves at the same time! Additionally, we'll want these \n",
      "light curves all scaled to the same mean and variance before applying the filter!**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "### extract summary statistics from the time series\n",
      "def timeseries_features(seg):\n",
      "    counts = seg[:,1] ## extract counts in full band\n",
      "    fmean = np.mean(counts)                    ## mean\n",
      "    fmedian = np.median(counts)                ## median\n",
      "    fvar = np.var(counts)                      ## variance\n",
      "    skew = scipy.stats.skew(counts)            ## skewness\n",
      "    kurt = scipy.stats.kurtosis(counts)        ## kurtosis\n",
      "    return fmean, fmedian, fvar, skew, kurt\n",
      "\n",
      "\n",
      "def linear_filter(counts_scaled, k=10):\n",
      "    \"\"\"\n",
      "    Extract features from a linear filter.\n",
      "    :param counts_scaled: numpy ndarray (nsamples, ntimebins) with SCALED TOTAL COUNTS\n",
      "    :return ww_all: numpy ndarray (nsamples, k) of weights \n",
      "    \"\"\"\n",
      "  \n",
      "    ## initialize the LinearFilter object\n",
      "    lf = linearfilter.LinearFilter(k=k)\n",
      "    \n",
      "    ww_all = np.zeros((len(counts_scaled), k))\n",
      "    \n",
      "    ## loop through all light curves and compute the weight vector for each\n",
      "    for i,c in enumerate(counts_scaled):\n",
      "        ww_all[i,:] = lf.fit_transform(c)[0]\n",
      "    return ww_all\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Fourier Domain Features\n",
      "\n",
      "We'll extract the integrated power in four power spectral bands, as well as power colours (ratios of these integrated PSD bands) and the maximum frequency at which we observe the maximum power (as a proxy for the presence of a QPO). \n",
      "The power spectral bands are taken from Lucy's paper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## boundaries for power bands\n",
      "pcb = {\"pa_min\":0.0039, \"pa_max\":0.031,\n",
      "       \"pb_min\":0.031, \"pb_max\":0.25,\n",
      "       \"pc_min\":0.25, \"pc_max\":2.0,\n",
      "       \"pd_min\":2.0, \"pd_max\":16.0}\n",
      "\n",
      "def rebin_psd(freq, ps, n=10, type='average'):\n",
      "\n",
      "    nbins = int(len(freq)/n)\n",
      "    df = freq[1] - freq[0]\n",
      "    T = freq[-1] - freq[0] + df\n",
      "    bin_df = df*n\n",
      "    binfreq = np.arange(nbins)*bin_df + bin_df/2.0 + freq[0]\n",
      "\n",
      "    #print(\"len(ps): \" + str(len(ps)))\n",
      "    #print(\"n: \" + str(n))\n",
      "\n",
      "    nbins_new = int(len(ps)/n)\n",
      "    ps_new = ps[:nbins_new*n]\n",
      "    binps = np.reshape(np.array(ps_new), (nbins_new, int(n)))\n",
      "    binps = np.sum(binps, axis=1)\n",
      "    if type in [\"average\", \"mean\"]:\n",
      "        binps = binps/np.float(n)\n",
      "    else:\n",
      "        binps = binps\n",
      "\n",
      "    if len(binfreq) < len(binps):\n",
      "        binps= binps[:len(binfreq)]\n",
      "\n",
      "    return binfreq, binps\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def psd_features(seg, pcb):\n",
      "    \"\"\"\n",
      "    Computer PSD-based features.\n",
      "    seg: data slice of type [times, count rates, count rate error]^T\n",
      "    pcb: frequency bands to use for power colours\n",
      "    \"\"\"\n",
      "\n",
      "    times = seg[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "\n",
      "    counts = seg[:,1]*dt\n",
      "    #ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "    freq, ps = make_psd(seg, navg=1)\n",
      "    #print(\"len(ps), before: \" + str(len(ps)))\n",
      "    if times[-1]-times[0] >= 2.*128.0:\n",
      "        tlen = (times[-1]-times[0])\n",
      "        nrebin = np.round(tlen/128.)\n",
      "        freq, ps = rebin_psd(freq, ps, n=nrebin, type='average')\n",
      "\n",
      "    #print(\"len(ps), after: \" + str(len(ps)))\n",
      "\n",
      "    freq = np.array(freq[1:])\n",
      "    ps = ps[1:]\n",
      "    #print(\"min(freq): \" + str(np.min(freq)))\n",
      "    #print(\"max(freq): \" + str(np.max(freq)))\n",
      "    #print(\"len(freq): \" + str(len(freq)))\n",
      "\n",
      "    binfreq, binps = total_psd(seg, 24)\n",
      "\n",
      "    #print(\"min(binps): \" + str(np.min(binps)))\n",
      "    #print(\"max(binps): \" + str(np.max(binps)))\n",
      "    fmax_ind = np.where(binps == np.max(binps))\n",
      "    #print(\"fmax_ind: \" + str(fmax_ind))\n",
      "    maxfreq = binfreq[fmax_ind[0]]\n",
      "    #print(\"maxfreq: \" + str(maxfreq))\n",
      "\n",
      "    ## find power in spectral bands for power-colours\n",
      "    pa_min_freq = freq.searchsorted(pcb[\"pa_min\"])\n",
      "    pa_max_freq = freq.searchsorted(pcb[\"pa_max\"])\n",
      "\n",
      "    pb_min_freq = freq.searchsorted(pcb[\"pb_min\"])\n",
      "    pb_max_freq = freq.searchsorted(pcb[\"pb_max\"])\n",
      "\n",
      "    pc_min_freq = freq.searchsorted(pcb[\"pc_min\"])\n",
      "    pc_max_freq = freq.searchsorted(pcb[\"pc_max\"])\n",
      "\n",
      "    pd_min_freq = freq.searchsorted(pcb[\"pd_min\"])\n",
      "    pd_max_freq = freq.searchsorted(pcb[\"pd_max\"])\n",
      "\n",
      "    psd_a = np.sum(ps[pa_min_freq:pa_max_freq])\n",
      "    psd_b = np.sum(ps[pb_min_freq:pb_max_freq])\n",
      "    psd_c = np.sum(ps[pc_min_freq:pc_max_freq])\n",
      "    psd_d = np.sum(ps[pd_min_freq:pd_max_freq])\n",
      "    pc1 = np.sum(ps[pc_min_freq:pc_max_freq])/np.sum(ps[pa_min_freq:pa_max_freq])\n",
      "    pc2 = np.sum(ps[pb_min_freq:pb_max_freq])/np.sum(ps[pd_min_freq:pd_max_freq])\n",
      "\n",
      "    return maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2\n",
      "\n",
      "def make_psd(segment, navg=1):\n",
      "\n",
      "    times = segment[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "\n",
      "    counts = segment[:,1]*dt\n",
      "\n",
      "    tseg = times[-1]-times[0]\n",
      "    nlc = len(times)\n",
      "    nseg = int(nlc/navg)\n",
      "\n",
      "    if navg == 1:\n",
      "        ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "        ps.freq = np.array(ps.freq)\n",
      "        ps.ps = np.array(ps.ps)*ps.freq\n",
      "        return ps.freq, ps.ps\n",
      "    else:\n",
      "        ps_all = []\n",
      "        for n in xrange(navg):\n",
      "            t_small = times[n*nseg:(n+1)*nseg]\n",
      "            c_small = counts[n*nseg:(n+1)*nseg]\n",
      "            ps = powerspectrum.PowerSpectrum(t_small, counts=c_small, norm=\"rms\")\n",
      "            ps.freq = np.array(ps.freq)\n",
      "            ps.ps = np.array(ps.ps)*ps.freq\n",
      "            ps_all.append(ps.ps)\n",
      "\n",
      "        #print(np.array(ps_all).shape)\n",
      "\n",
      "        ps_all = np.average(np.array(ps_all), axis=0)\n",
      "\n",
      "        #print(ps_all.shape)\n",
      "\n",
      "    return ps.freq, ps_all\n",
      "\n",
      "epsilon = 1.e-8\n",
      "\n",
      "def total_psd(seg, bins):\n",
      "    times = seg[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "    counts = seg[:,1]*dt\n",
      "\n",
      "    ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "    ps.ps = np.array(ps.freq)*np.array(ps.ps)\n",
      "    binfreq = np.logspace(np.log10(ps.freq[1]-epsilon), np.log10(ps.freq[-1]+epsilon), bins)\n",
      "    #print(\"freq: \" + str(ps.freq[1:10]))\n",
      "    #print(\"binfreq: \" + str(binfreq[:10]))\n",
      "    binps, bin_edges, binno = scipy.stats.binned_statistic(ps.freq[1:], ps.ps[1:], statistic=\"mean\", bins=binfreq)\n",
      "    df = binfreq[1:]-binfreq[:-1]\n",
      "    binfreq = binfreq[:-1]+df/2.\n",
      "\n",
      "    return np.array(binfreq), np.array(binps)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_psd(segment, navg=1):\n",
      "    \n",
      "    times = segment[:,0]\n",
      "    dt = times[1:] - times[:-1]\n",
      "    dt = np.min(dt)\n",
      "\n",
      "    counts = segment[:,1]*dt\n",
      "    \n",
      "    tseg = times[-1]-times[0]\n",
      "    nlc = len(times)\n",
      "    nseg = int(nlc/navg) \n",
      "    \n",
      "    if navg == 1:\n",
      "        ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
      "        return ps.freq, ps.ps\n",
      "    else:\n",
      "        ps_all = []\n",
      "        for n in xrange(navg):\n",
      "            t_small = times[n*nseg:(n+1)*nseg]\n",
      "            c_small = counts[n*nseg:(n+1)*nseg]\n",
      "            ps = powerspectrum.PowerSpectrum(t_small, counts=c_small, norm=\"rms\")\n",
      "            ps_all.append(ps.ps)\n",
      "        \n",
      "        #print(np.array(ps_all).shape) \n",
      "    \n",
      "        ps_all = np.average(np.array(ps_all), axis=0)\n",
      "\n",
      "        #print(ps_all.shape) \n",
      "    \n",
      "        return ps.freq, ps_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Energy Domain Features\n",
      "\n",
      "Hardness ratios, the ratio of the flux in one photon energy band versus the flux in another, is a typical and useful proxy for more detailed energy spectral modelling. X-ray binaries are well known for their changing energy spectra in different stages of outburst, and GRS 1915+105 is no exception. Any features relating to hardness ratios are useful not only because they are likely to be discriminative, but also because they have a direct link to physical processes in the source, and are directly interpretable to physicists. \n",
      "\n",
      "The *Rossi* X-ray Timing Explorer (RXTE for short) covers the energy range between 2 and 70 keV. Our data is split into four bands, one covering the whole range, and three that covers the lower range, midrange and upper range of the energy bands (need numbers here!). Hardness ratios are generally derived by dividing the flux in the midrange and upper range by the flux in the lower energy band. These are called hardness ratio 1 and 2, respectively (need to add exact definitions). \n",
      "In general, a scatter plot of one hardness ratio against another (an X-ray colour-colour diagram) for a time series reveals that in different states, hardness ratios can differ by a facter of 10 or more. In some states, the source also makes a certain track in the colour-colour diagram. \n",
      "\n",
      "Our task is to summarize this behaviour in as few features as possible. \n",
      "The approach we choose here is particularly simple: we compute means of both hardness ratios for each segment, as well as the covariance between the two. This gives us a total of five numbers: two means, two variances (diagonal of the covariance matrix) and one covariance (the other is symmetric to the first).\n",
      "This will likely **not** capture the full behavious, because for some states, the colour-colour diagram isn't particularly well modelled by a 2D Gaussian, but our hope is that it will be good enough for the classification tasks attempted here. We also extract skewness and kurtosis for both hardness ratios, which we hope will capture some of the non-Gaussianities in the hardness ratios.\n",
      "\n",
      "We can also experiment with more complex representations (for example a 2D histogram representation) or a bivariate Gaussian distribution fit, but initial experiments suggest that these representations do not particularly increase recall or precision in classification tasks.\n",
      "\n",
      "The code is still here for completeness, as is a function that computes a 2D histogram of the hardness-intensity diagram (HID), another diagnostic often used in X-ray binary research, but that does not seem to add anything to classification tasks, either."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_hrlimits(hr1, hr2):\n",
      "    min_hr1 = np.min(hr1)\n",
      "    max_hr1 = np.max(hr1)\n",
      "\n",
      "    min_hr2 = np.min(hr2)\n",
      "    max_hr2 = np.max(hr2)\n",
      "    return [[min_hr1, max_hr1], [min_hr2, max_hr2]]\n",
      "\n",
      "def hr_maps(seg, bins=30, hrlimits=None):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    mid_counts = seg[:,3]\n",
      "    high_counts = seg[:,4]\n",
      "    hr1 = np.log(mid_counts/low_counts)\n",
      "    hr2 = np.log(high_counts/low_counts)\n",
      "\n",
      "    if hrlimits is None:\n",
      "        hr_limits = compute_hrlimits(hr1, hr2)\n",
      "    else:\n",
      "        hr_limits = hrlimits\n",
      "\n",
      "    h, xedges, yedges = np.histogram2d(hr1, hr2, bins=bins,\n",
      "                                       range=hr_limits)\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "    hmax = np.max(h)\n",
      "    #print(hmax)\n",
      "    hmask = np.where(h > hmax/20.)\n",
      "    hmask1 = np.where(h < hmax/20.)\n",
      "    hnew = copy.copy(h)\n",
      "    hnew[hmask[0], hmask[1]] = 1.\n",
      "    hnew[hmask1[0], hmask1[1]] = 0.0\n",
      "    return xedges, yedges, hnew\n",
      "\n",
      "def hr_fitting(seg):\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    mid_counts = seg[:,3]\n",
      "    high_counts = seg[:,4]\n",
      "    hr1 = mid_counts/low_counts\n",
      "    hr2 = high_counts/low_counts\n",
      "\n",
      "    hr1_mask = np.where(np.isinf(hr1) == False)\n",
      "    hr1 = hr1[hr1_mask]\n",
      "    hr2 = hr2[hr1_mask]\n",
      "\n",
      "    hr2_mask = np.where(np.isinf(hr2) == False)\n",
      "    hr1 = hr1[hr2_mask]\n",
      "    hr2 = hr2[hr2_mask]\n",
      "\n",
      "    mu1 = np.mean(hr1)\n",
      "    mu2 = np.mean(hr2)\n",
      "\n",
      "    cov = np.cov(hr1, hr2).flatten()\n",
      "\n",
      "    skew = scipy.stats.skew(np.array([hr1, hr2]).T)\n",
      "    kurt = scipy.stats.kurtosis(np.array([hr1, hr2]).T)\n",
      "\n",
      "    if np.any(np.isnan(cov)):\n",
      "        print(\"NaN in cov\")\n",
      "\n",
      "    return mu1, mu2, cov, skew, kurt\n",
      "\n",
      "def hid_maps(seg, bins=30):\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr2 = high_counts/low_counts\n",
      "    hid_limits = compute_hrlimits(hr2, counts)\n",
      "\n",
      "    h, xedges, yedges = np.histogram2d(hr2, counts, bins=bins,\n",
      "                                       range=hid_limits)\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "\n",
      "    return xedges, yedges, h\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below, for completeness, are another few snippets of code that extracts binned versions of the light curve in case I ever want to plug these into a classifier (but I generally don't) as well as a piece of code that extracts the (1) full light curve, (2) hardness ratio 1 and (3) hardness ratio 2. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lcshape_features(seg, dt=1.0):\n",
      "    \n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "\n",
      "    dt_small = times[1:]-times[:-1]\n",
      "    dt_small = np.min(dt_small)\n",
      "\n",
      "    nbins = np.round(dt/dt_small)\n",
      "\n",
      "    bintimes, bincounts = rebin_psd(times, counts, nbins)\n",
      "    \n",
      "    return bincounts\n",
      "\n",
      "def extract_lc(seg):\n",
      "    times = seg[:,0]\n",
      "    counts = seg[:,1]\n",
      "    low_counts = seg[:,2]\n",
      "    high_counts = seg[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    return [times, counts, hr1, hr2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Making Features\n",
      "\n",
      "Now we can use this code to extract some features and put them into a giant vector.\n",
      "We will need to extract the time series features using the linear filter separately, because this involves scaling *all* light curves to unit mean and variance, and then using `LinearFilterEnsemble` on the entire thing. \n",
      "So, we'll first extract all other features, then we'll extract the features from the linear filter, and then we'll stack them all together. Easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_features(seg, k=10, bins=30, navg=4, hr_summary=True, ps_summary=True, lc=True, hr=True, hrlimits=None):\n",
      "    \"\"\"\n",
      "    Make features from a set of light curve segments, except for the linear filter!\n",
      "    \n",
      "    :param seg: list of all segments to be used\n",
      "    :param bins: bins used in a 2D histogram if hr_summary is False\n",
      "    :param hr_summary: if True, summarize HRs in means and covariance matrix\n",
      "    :param ps_summary: if True, summarize power spectrum in frequency of maximum power and power spectral bands\n",
      "    :param lc: if True, store light curves\n",
      "    :param hr: if True, store hardness ratios\n",
      "    :param hrlimits: limits for the 2D histogram if hr_summary is False\n",
      "    :return: fdict: dictionary with keywords \"features\", \"lc\" and \"hr\"\n",
      "    \"\"\"\n",
      "    features = []\n",
      "    if lc:\n",
      "        lc_all = []\n",
      "    if hr:\n",
      "        hr_all = []\n",
      "        \n",
      "        \n",
      "    ## MAKE FEATURES BASED ON LINEAR FILTER\n",
      "\n",
      "    ## first, extract the total counts out of each segment\n",
      "    counts = np.array([s[:,1] for s in seg])\n",
      "\n",
      "    ## next, transform the array such that *each light curve* is scaled\n",
      "    ## to zero mean and unit variance\n",
      "    ## We can do this for all light curves independently, because we're\n",
      "    ## averaging *per light curve* and not *per time bin*\n",
      "    counts_scaled = StandardScaler().fit_transform(counts.T).T\n",
      "\n",
      "    ## transform the counts into a weight vector\n",
      "    ww = linear_filter(counts_scaled, k=k)\n",
      "    \n",
      "    for s in seg:\n",
      "\n",
      "        features_temp = []\n",
      "\n",
      "        ## time series summary features\n",
      "        fmean, fmedian, fvar, fskew, fkurt = timeseries_features(s)\n",
      "        features_temp.extend([fmean, fmedian, fvar, fskew, fkurt])\n",
      "\n",
      "\n",
      "        if ps_summary:\n",
      "            ## PSD summary features\n",
      "            maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2 = psd_features(s, pcb)\n",
      "            features_temp.extend([maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2])\n",
      "        else:\n",
      "            ## whole PSD\n",
      "            #freq, ps = make_psd(s,navg=navg)\n",
      "            binfreq, binps = total_psd(s, 24)\n",
      "            features_temp.extend(binps[1:])\n",
      "\n",
      "\n",
      "        if hr_summary:\n",
      "            mu1, mu2, cov, skew, kurt = hr_fitting(s)\n",
      "            #print(\"len(features): \" + str(len(features_temp)))\n",
      "            features_temp.extend([mu1, mu2])\n",
      "            #print(len(features_temp))\n",
      "            features_temp.extend([cov[0], cov[1], cov[3]])\n",
      "            #print(len(features_temp))\n",
      "            features_temp.extend(list(skew))\n",
      "            #print(len(features_temp))\n",
      "            features_temp.extend(list(kurt))\n",
      "            #print(len(features_temp))\n",
      "\n",
      "\n",
      "        else:\n",
      "            xedges, yedges, h = hr_maps(s, bins=bins, hrlimits=hrlimits)\n",
      "            features_temp.extend(h.flatten())\n",
      "\n",
      "        features.append(features_temp)\n",
      "    \n",
      "\n",
      "        if lc or hr:\n",
      "            lc_temp = extract_lc(s)\n",
      "        if lc:\n",
      "            #print(\"appending light curve\")\n",
      "            lc_all.append([lc_temp[0], lc_temp[1]])\n",
      "        if hr:\n",
      "            #print(\"appending hardness ratios\")\n",
      "            hr_all.append([lc_temp[2], lc_temp[3]])\n",
      "\n",
      "    features_all = np.hstack((np.array(features), ww))\n",
      "    \n",
      "    print(\"I am about to make a dictionary\")\n",
      "    fdict = {\"features\": features_all}\n",
      "    print(fdict.keys)\n",
      "    if lc:\n",
      "        print(\"I am in lc\")\n",
      "        #features.append(lc_all)\n",
      "        fdict[\"lc\"] = lc_all\n",
      "    if hr:\n",
      "        print(\"I am in hr\")\n",
      "        #features.append(hr_all)\n",
      "        fdict[\"hr\"] = hr_all\n",
      "    #print(fdict.keys())\n",
      "    return fdict\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can make the dictionaries with features, light curves and hardness ratios:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train = make_features(seg_train[:6], bins=30)\n",
      "features_val = make_features(seg_val[:4], bins=30)\n",
      "features_test = make_features(seg_test[:5], bins=30)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I am about to make a dictionary\n",
        "<built-in method keys of dict object at 0x172a997f8>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary\n",
        "<built-in method keys of dict object at 0x172c1c4b0>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x172c1c5c8>\n",
        "I am in lc\n",
        "I am in hr\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The order of the features is the following:\n",
      "\n",
      "\n",
      "1. mean\n",
      "2. median\n",
      "3. variance\n",
      "4. skewness\n",
      "5. kurtosis\n",
      "\n",
      "If `ps_summary` is set to `True`, then the list continues:\n",
      "\n",
      "6. frequency where the periodogram has its maximum power\n",
      "7. integrated power in PSD band A\n",
      "8. integrated power in PSD band B\n",
      "9. integrated power in PSD band C\n",
      "10. integrated power in PSD band D\n",
      "11. Power Colour 1: PSD C / PSD A\n",
      "12. Power Colour 2: PSD B / BSD D\n",
      "\n",
      "otherwise features 6-30 are the power in 24 power spectral bins.\n",
      "\n",
      "If `hr_summary` is set to `True`, then the list continues:\n",
      "\n",
      "13. mean of HR 1\n",
      "14. mean of HR 2\n",
      "15. variance of HR 1\n",
      "16. covariance between HR 1 and HR 2 \n",
      "17. variance of HR 2\n",
      "18. skewness of HR 1\n",
      "19. skewness of HR 2\n",
      "20. kurtosis of HR 1\n",
      "21. kurtosis of HR 2\n",
      "\n",
      "otherwise, the next set of features is an `(nbins x nbins)` long list\n",
      "of bins from a 2D histogram representation.\n",
      "\n",
      "The last `[-k:]` features contain the `k`-long weight vector for each \n",
      "light curve describing the linear filter.\n",
      "\n",
      "\n",
      "### Saving the data for future use\n",
      "\n",
      "Let's save our features in a bunch of files for future use. \n",
      "Because the feature matrices are not that big, they go into a simple ascii file.\n",
      "Light curves and hardness ratios go into pickle files. Let's define a little helper\n",
      "function to make this easier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Remember how we're doing the longer segments right now? \n",
      "## Yeah, don't forget that in the file name!\n",
      "\n",
      "\n",
      "def save_features(fdict, seg_length, lc=True, hr=True, froot=\"grs1915\"):\n",
      "    froot = froot + str(int(seg_length))\n",
      "    np.savetxt(froot + \"_features.txt\", fdict[\"features\"])\n",
      "    if lc:\n",
      "        f = open(froot + \"_lightcurves.dat\", \"w\")\n",
      "        pickle.dump(fdict[\"lc\"], f)\n",
      "        f.close()\n",
      "    if hr:\n",
      "        f = open(froot + \"_hardness.dat\", \"w\")\n",
      "        pickle.dump(fdict[\"hr\"], f)\n",
      "        f.close()\n",
      "        \n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_features(features_train, seg_length_supervised, froot=\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combining it all\n",
      "\n",
      "The code above starts with a file called `grs1915_all_125ms.dat`. This file \n",
      "has the data extracted from their original fits files and separated observations\n",
      "into individual light curves split up by data gaps. \n",
      "\n",
      "First, we define a helper function that goes through each observation, finds data gaps and splits the data along these gaps into uneven segments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import generaltools as gt\n",
      "\n",
      "## split light curves into segments without gaps\n",
      "def remove_gaps(d, state):\n",
      "        \n",
      "    dt_data = d[1:,0]-d[:-1,0]\n",
      "    dt_min = np.min(dt_data)\n",
      "    print(\"dt_min: \" + str(dt_min))\n",
      "    tol = dt_min*0.01\n",
      "    \n",
      "    ### split data with breaks\n",
      "    breaks = np.where(dt_data > dt_min+tol)[0]\n",
      "    #print(breaks)\n",
      "    d_all = []\n",
      "    \n",
      "    if len(breaks) == 0:\n",
      "        dtemp = d\n",
      "        d_all.append([dtemp, state])\n",
      "    else:\n",
      "        for i,b in enumerate(breaks):\n",
      "            #print(\"Break in light curve at time bin %i; length of break dt = %.3f\"%(b, dt_data[b]))\n",
      "            if i == 0:\n",
      "                if b == 0:\n",
      "                    #print(\"First break is at first time bin\")\n",
      "                    continue\n",
      "                else:\n",
      "                    #print(\"I am extracting the data before the break\")\n",
      "                    dtemp = d[:b]\n",
      "\n",
      "            else:\n",
      "                #print(\"I am extracting data after break \" + str(i))\n",
      "                dtemp = d[breaks[i-1]+1:b]\n",
      "\n",
      "            d_all.append([dtemp, state])\n",
      "\n",
      "        ## last segment\n",
      "        #print(\"I am computing last segment of file\")\n",
      "        dtemp = d[b+1:]\n",
      "        d_all.append([dtemp, state])\n",
      "\n",
      "    return d_all\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We might also need a function that rebins the data into light curves with a coarser \n",
      "time resolution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## helper function that bins light curves\n",
      "def bin_lightcurve(dtemp, nbins):\n",
      "    tbinned_times, tbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,1], n=nbins, type=\"average\")\n",
      "    #print(\"dt: \" + str(tbinned_times[1]-tbinned_times[0]))\n",
      "    lbinned_times, lbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,2], n=nbins, type=\"average\")\n",
      "    hbinned_times, hbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,3], n=nbins, type=\"average\")\n",
      "    dshort = np.transpose(np.array([tbinned_times, tbinned_counts, lbinned_counts, hbinned_counts]))\n",
      "    return dshort\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes, for whatever reason, `inf`s and `nan`s might sneak into our feature vectors. \n",
      "This will make the Standard Scaler (and various machine learning algorithms) very unhappy,\n",
      "so we'd like to remove them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_nan(features, labels, hr=True, lc=True):\n",
      "    inf_ind = []\n",
      "    fnew, lnew, tnew = [], [], []\n",
      "    if lc:\n",
      "        lcnew = []\n",
      "    if hr:\n",
      "        hrnew = []\n",
      "\n",
      "    for i,f in enumerate(features[\"features\"]):\n",
      "\n",
      "        try:\n",
      "            if any(np.isnan(f)):\n",
      "                print(\"NaN in sample row %i\"%i)\n",
      "                continue\n",
      "            elif any(np.isinf(f)):\n",
      "                print(\"inf sample row %i\"%i)\n",
      "                continue\n",
      "            else:\n",
      "                fnew.append(f)\n",
      "                lnew.append(labels[i])\n",
      "                tnew.append(features[\"tstart\"][i])\n",
      "                if lc:\n",
      "                    lcnew.append(features[\"lc\"][i])\n",
      "                if hr:\n",
      "                    hrnew.append(features[\"hr\"][i])\n",
      "        except ValueError:\n",
      "            print(\"f: \" + str(f))\n",
      "            print(\"type(f): \" + str(type(f)))\n",
      "            raise Exception(\"This is breaking! Boo!\")\n",
      "    features_new = {\"features\":fnew, \"tstart\":tnew}\n",
      "    if lc:\n",
      "        features_new[\"lc\"] = lcnew\n",
      "    if hr:\n",
      "        features_new[\"hr\"] = hrnew\n",
      "    return features_new, lnew\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below is the function that actually does all the feature extraction all in one step. Note that for extracting different features, you'll need to change those in the function below. I might make that more robust in the future."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import convert_belloni \n",
      "\n",
      "def make_all_features(d_all, k=10, lamb=0.1, val=True, train_frac=0.6, validation_frac=0.2, test_frac = 0.2,\n",
      "                  seg=True, seg_length=1024., overlap = 64.,\n",
      "                  bins=30, navg=4, hr_summary=True, ps_summary=True, lc=True, hr=True,\n",
      "                  save_features=True, froot=\"grs1915\"):\n",
      "\n",
      "    ## Set the seed to I will always pick out the same light curves.\n",
      "    np.random.seed(20150608)\n",
      "\n",
      "    ## shuffle list of light curves\n",
      "    np.random.shuffle(d_all)\n",
      "\n",
      "    ## let's pull out light curves for three data sets into different variables.\n",
      "    d_all_train = d_all[:int(train_frac*n_lcs)]\n",
      "    d_all_test = d_all[int(train_frac*n_lcs):int((train_frac + test_frac)*n_lcs)]\n",
      "\n",
      "    seg_train, labels_train = extract_segments(d_all_train, seg_length=seg_length, overlap=overlap)\n",
      "    seg_test, labels_test = extract_segments(d_all_test, seg_length=seg_length, overlap=overlap)\n",
      "\n",
      "    if val:\n",
      "        d_all_val = d_all[int((train_frac + test_frac)*n_lcs):]\n",
      "        seg_val, labels_val = extract_segments(d_all_val, seg_length=seg_length, overlap=overlap)\n",
      "\n",
      "\n",
      "    ### hrlimits are derived from the data, in the GRS1915_DataVisualisation Notebook\n",
      "    hrlimits = [[-2.5, 1.5], [-3.0, 2.0]]\n",
      "\n",
      "    features_train = make_features(seg_train,k, bins, lamb, hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
      "    features_test = make_features(seg_test,k, bins,  lamb,hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
      "\n",
      "    print(\"len(tstart_train): \" + str(len(tstart_train)))\n",
      "    print(\"len(features_train): \" + str(len(features_train)))\n",
      "\n",
      "    features_train[\"tstart\"] = tstart_train\n",
      "    features_test[\"tstart\"] = tstart_test\n",
      "    #features_train = np.concatenate((tstart_train, features_train))\n",
      "    #features_test = np.concatenate((tstart_test, features_test))\n",
      "\n",
      "    print(\"len(tstart_test): \" + str(len(tstart_test)))\n",
      "    print(\"len(features_test): \" + str(len(features_test)))\n",
      "\n",
      "\n",
      "    ## check for NaN\n",
      "    print(\"Checking for NaN in the training set ...\")\n",
      "    features_train_checked, labels_train_checked = check_nan(features_train, labels_train, hr=hr, lc=lc)\n",
      "    print(\"Checking for NaN in the test set ...\")\n",
      "    features_test_checked, labels_test_checked = check_nan(features_test, labels_test, hr=hr, lc=lc)\n",
      "\n",
      "\n",
      "    labelled_features = {\"train\": [features_train_checked[\"features\"], labels_train_checked],\n",
      "                     \"test\": [features_test_checked[\"features\"], labels_test_checked]}\n",
      "\n",
      "    if val:\n",
      "        features_val = make_features(seg_val, k, bins, lamb, hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
      "        #features_val = np.concatenate((tstart_val, features_val))\n",
      "        features_val[\"tstart\"] = tstart_val\n",
      "\n",
      "        features_val_checked, labels_val_checked = check_nan(features_val, labels_val, hr=hr, lc=lc)\n",
      "\n",
      "        labelled_features[\"val\"] =  [features_val_checked[\"features\"], labels_val_checked],\n",
      "\n",
      "    print(\"Checking for NaN in the validation set ...\")\n",
      "\n",
      "    if save_features:\n",
      "        np.savetxt(froot+\"%i_features_train.txt\"%int(seg_length), features_train_checked[\"features\"])\n",
      "        np.savetxt(froot+\"%i_features_test.txt\"%int(seg_length), features_test_checked[\"features\"])\n",
      "\n",
      "        np.savetxt(froot+\"%i_tstart_train.txt\"%int(seg_length), features_train_checked[\"tstart\"])\n",
      "        np.savetxt(froot+\"%i_tstart_test.txt\"%int(seg_length), features_test_checked[\"tstart\"])\n",
      "\n",
      "        ltrainfile = open(froot+\"%i_labels_train.txt\"%int(seg_length), \"w\")\n",
      "        for l in labels_train_checked:\n",
      "            ltrainfile.write(str(l) + \"\\n\")\n",
      "        ltrainfile.close()\n",
      "\n",
      "        ltestfile = open(froot+\"%i_labels_test.txt\"%int(seg_length), \"w\")\n",
      "        for l in labels_test_checked:\n",
      "            ltestfile.write(str(l) + \"\\n\")\n",
      "        ltestfile.close()\n",
      "\n",
      "\n",
      "        if val:\n",
      "            np.savetxt(froot+\"%i_features_val.txt\"%int(seg_length), features_val_checked[\"features\"])\n",
      "            np.savetxt(froot+\"%i_tstart_val.txt\"%int(seg_length), features_val_checked[\"tstart\"])\n",
      "\n",
      "            lvalfile = open(froot+\"%i_labels_val.txt\"%int(seg_length), \"w\")\n",
      "            for l in labels_val_checked:\n",
      "                lvalfile.write(str(l) + \"\\n\")\n",
      "            lvalfile.close()\n",
      "\n",
      "\n",
      "        if lc:\n",
      "            lc_all = {\"train\":features_train[\"lc\"], \"test\":features_test[\"lc\"]}\n",
      "            if val:\n",
      "                lc_all[\"val\"] = features_val[\"lc\"]\n",
      "\n",
      "            f = open(froot+\"%i_lc_all.dat\"%int(seg_length), \"w\")\n",
      "            pickle.dump(lc_all, f, -1)\n",
      "            f.close()\n",
      "\n",
      "        if hr:\n",
      "            hr_all = {\"train\":features_train[\"hr\"], \"test\":features_test[\"hr\"]}\n",
      "            if val:\n",
      "                hr_all[\"val\"] = features_val[\"hr\"]\n",
      "\n",
      "            f = open(froot+\"%i_hr_all.dat\"%int(seg_length), \"w\")\n",
      "            pickle.dump(hr_all, f, -1)\n",
      "            f.close()\n",
      "\n",
      "        #f = open(froot+\"_features.dat\", \"w\")\n",
      "        #pickle.dump(labelled_features, f)\n",
      "        #f.close()\n",
      "\n",
      "    return labelled_features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using the Script\n",
      "\n",
      "For the actual analysis, I have put the stuff above into a script `feature_extraction.py`. \n",
      "We will run it on segments with the following choices determined by cross validation and \n",
      "common sense (see also feature engineering notebook):\n",
      "\n",
      "1. segments are 1024 seconds long (for supervised classification) and 256 seconds for unsupervised classification\n",
      "2. the overlap between consecutive segments is 64 seconds\n",
      "3. the fraction of training data is 0.5, those of validation and test data 0.25\n",
      "4. the number of weights, $k=$ for the linear filter (cross validation)\n",
      "5. the regularization parameter for ridge regression in the linear filter, is $\\alpha=$ (cross validation)\n",
      "\n",
      "More to come!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feature_extraction\n",
      "datadir = \"/Volumes/Lliarinh/data/grs1915/\"\n",
      "feature_extraction.extract_all(d_all, seg_length_all=[256., 1024.], overlap=128.,\n",
      "                val=True, train_frac=0.5, validation_frac = 0.25, test_frac = 0.25,\n",
      "                datadir=\"./\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "256 segments, summary\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b3387f8>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b333398>\n",
        "I am in lc\n",
        "I am in hr\n",
        "len(tstart_train): 15707\n",
        "len(features_train): 3\n",
        "len(tstart_test): 7738\n",
        "len(features_test): 4\n",
        "Checking for NaN in the training set ...\n",
        "Checking for NaN in the test set ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NaN in cov"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b32cd70>\n",
        "I am in lc\n",
        "I am in hr\n",
        "NaN in sample row 5122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Checking for NaN in the validation set ...\n",
        "1024 segments, summary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b32cd70>\n",
        "I am in lc\n",
        "I am in hr\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b32c910>\n",
        "I am in lc\n",
        "I am in hr\n",
        "len(tstart_train): 8957"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "len(features_train): 3\n",
        "len(tstart_test): 4214\n",
        "len(features_test): 4\n",
        "Checking for NaN in the training set ...\n",
        "Checking for NaN in the test set ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "I am about to make a dictionary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<built-in method keys of dict object at 0x10b3387f8>\n",
        "I am in lc\n",
        "I am in hr\n",
        "Checking for NaN in the validation set ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "u'/Users/danielahuppenkothen/work/data/grs1915/BlackHoleMagic/code'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}