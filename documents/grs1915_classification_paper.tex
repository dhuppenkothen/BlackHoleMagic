% This document is part of the transientdict project.
% Copyright 2013 the authors.

\documentclass[12pt]{emulateapj}
\usepackage{graphicx}
%\usepackage{epsfig}
\usepackage{times}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{url}
%\usepackage{subfigure}
\usepackage{microtype}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{subfigure}


%\usepackage{longtable}%\usepackage[stable]{footmisc}
%\usepackage{color}
%\bibliographystyle{apj}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\fermi}{\project{Fermi}}
\newcommand{\rxte}{\project{RXTE}}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\counts}{y}
\newcommand{\pars}{\theta}
\newcommand{\mean}{\lambda}
\newcommand{\likelihood}{{\mathcal L}}
\newcommand{\Poisson}{{\mathcal P}}
\newcommand{\Uniform}{{\mathcal U}}
\newcommand{\bg}{\mathrm{bg}}
\newcommand{\word}{\phi}


%\newcommand{\bs}{\boldsymbol}

\begin{document}

\title{Black Hole Classification Magic}

\author{Daniela Huppenkothen\altaffilmark{1, 2, 3}, Lucy M. Heil\altaffilmark{4}, David W. Hogg\altaffilmark{2,1}}
 
  \altaffiltext{1}{Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY 10003}
  \altaffiltext{2}{Center for Cosmology and Particle Physics, Department of Physics, New York University, 4 Washington Place, New York, NY 10003, USA}
  \altaffiltext{3}{E-mail: daniela.huppenkothen@nyu.edu}
  \altaffiltext{4}{Anton Pannekoek Institute for Astronomy, University of
  Amsterdam, Postbus 94249, 1090 GE Amsterdam, the Netherlands}
%  \altaffiltext{5}{Department of Statistics, The University of Auckland, Private Bag 92019, Auckland 1142, New Zealand}
%\altaffiltext{6}{School of Informatics, University of Edinburgh}
%\altaffiltext{7}{School of Engineering and Computer Science, Victoria University of Wellington, New Zealand}
%\altaffiltext{8}{Monash Center for Astrophysics and School of Physics, Monash University, Clayton, Victoria 3800, Australia}
%\altaffiltext{9}{Astrophysics Office, ZP 12, NASA/Marshall Space Flight Center, Huntsville, AL 35812, USA}
%\altaffiltext{10}{NSSTC, 320 Sparkman Drive, Huntsville, AL 35805, USA}


\begin{abstract}
.

\end{abstract}

%\keywords{pulsars: individual (SGR J1550-5418), stars: magnetic fields, stars: neutron, X-rays: bursts, methods:statistics}

\section{Introduction}

Some stuff about black holes. Also, perhaps the total ASM light curve.

\section{Observations and Data Preparation}

{\bf Lucy: Please add some stuff about data modes and extraction and stuff? Also, restrictions on which observations were used etc.}

\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{grs1915_asm_lc_all.pdf}
\caption{\rxte\ All-Sky Monitor (ASM) light curve for the entire duration of the \rxte\ mission. Each panel covers $500$ days. Shown in blue is the ASM light curve. In green, the start points of the \rxte/PCA observations with high enough time resolution to be relevant for this analysis. The \rxte/PCA observations span the entire lifetime and provide a 
sample with high coverage in time, albeit with a bias toward active periods of the system.}
\label{fig:asm_total}
\end{center}
\end{figure*}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=9cm]{obs_duration.pdf}
\caption{Histogram of the durations of all observations used in the analysis. Most observations have durations of $1000$ --- $5000$ seconds, few are significantly longer. Note that these reflect total durations for a given observation without application of Good Time Intervals (GTIs); in the analysis below, these durations may be shortened or split in parts by detector failures and the motion of the space craft ({\bf Lucy: I think there is something about the RXTE orbit that gives observations a natural upper limit?}.}
\label{fig:asm_total}
\end{center}
\end{figure}


We extracted light curves in $4$ energy bands: $3 - 13$ keV, $3 - 6$ keV, $9 - 15$ keV, and $15 - 75$ keV. While the energy ranges will not be exactly the same from light curve to light curve due to the gradual changes in the sensitivity of individual channels over time, we have taken care to keep the energy ranges as constant as possible, including and excluding channels as necessary. All light curves have a time resolution of $0.125$ seconds. We include a total of $1711$ observations in the analysis. 
We split the observations in training, validation and test data sets, with $60\%$ of observations in the training set and $20\%$ of all observations in the validation and test sets each. 
Each observation is split into segments of $1024\,\mathrm{s}$ duration, starting every $128\,\mathrm{s}$ apart. This leads to overlaps between consecutive segments, but augments the relatively small data set and ensures that we are sensitive to within-state changes in light curve properties. We note that we repeated the analysis below with both shorter and longer segment sizes. For shorter segment sizes, changes in the light curve and energy properties within states leads to increased uncertainty in the classification, for longer segments, the training set becomes too small to properly train the model, thus leading to an increase in mis-classifications.

We use the $3 - 13$ keV band for all time series and power spectral features, and form two hardness ratios that encode energy spectral changes between states. Hardness ratio 1 (HR1) is defined as $\mathrm{HR}1 = $.

\section{Feature Engineering and Supervised Classification}

While some machine learning algorithms can produce reliable classifications on raw data (e.g. a light curve), these algorithms generally require a very large amount of data of the order of tens of millions of data points or more. 
For most problems in X-ray astronomy, there is not enough data available to make using these methods feasible. Algorithms that work on relatively small data sets exist, but they generally work better with data with fewer dimensions. Therefore, we extract \textit{features} for each data \textit{sample}. 
Here, a sample is a single instance of the ensemble to be classified, in our case an RXTE light curve of GRS1915+105, or perhaps a short interval of a light curve. Depending on the length of that interval or light curve, the raw data might have tens of thousands of data points, corresponding to a high-dimensional vector in feature space. We reduce the number of dimensions by extracting features, descriptive summaries of the raw data that will allow for efficient separation of the various classes in feature-space. 

Feature engineering is the most important and most difficult part of any machine learning problem. It is here where domain knowledge of the problem at hand becomes crucial to finding the most informative features to be used by the computer in the subsequent classification task. 
We used the previous (human-based) classification by \citet{belloni2000} to guide the feature engineering task. With relatively high-resolution light curves ($\Delta t = 0.125 \,\mathrm{s}$) in four energy bands, there is a multitude of possible features in time, energy and frequency domains that could potentially inform our choices. Because \citet{belloni2000} based their classification on the hardness ratios and overall appearance of the light curves, we start with similar reasoning and supplement the feature set derived from the time series and hardness ratios with properties of the power spectrum. 

Ideally, we would like to use segments of the same length in order to avoid systematic biases in our features (which, in the case of summary statistics, might depend on the number of data points in the light curve) and because some features are structured such that light curves of different duration give feature vectors of different lengths, making the later classification task vastly more complex. The ideal duration of the segments to be used is an open \testit{hyperparameter} in the problem, and thus has to be estimated during the classification procedure. There is a trade-off between descriptiveness and sample completeness: when choosing long segments, we likely encapsulate more of the characteristic behaviour of a state, which can sometimes consist of cycles lasting more than a thousand seconds. On the other hand, if we choose long segments, we necessarily exclude all light curves that are shorter than that, for example because their Good Time Intervals (GTIs) only allowed for shorter segments. We adopt an approach where we test our supervised learning algorithms below with segments varying in duration from $16\,\mathrm{s}$ to $1024\,\mathrm{s}$ with a procedure laid out in more detail in Section \ref{sec:freeparams}.

\subsection{Time Series Features}

Because it is difficult to encapsulate the large variety of shapes observed in the light curves of GRS 1915+105, we use a mix of very simple summary features and extract a number of features from a linear model. The summary features are: the mean count rate, median count rate, total variance, skewness and kurtosis in the light curve segment in the $3 - 13$ keV band. 

The light curves observed from GRS1915+105 show a very rich variability behaviour that includes complex patterns not well represented by the summary features listed above. Encapsulating these complex variability patterns in a few parameters is difficult for a variety of reasons. Any method must be able to encode complex patterns in just a few parameters. At the same time, it must be shift-invariant. That is, for roughly periodic patterns, features should look very similar regardless of where in the cycle a light curve begins. We attempt to encapsulate the variability in a simple linear model, where the data $y_t$ at any given point in the light curve $t$ depends on a linear combination of $k$ data points immediately before:

\begin{equation}
y_{t+1} = \langle w, X_{t+1} \rangle \, ,
\end{equation}

\noindent  where $w$ is a vector of lengths $k$ specifying the weights and $X_{t+1} = y_{t-k:t}$ is a vector containing the data points between steps $t-k$ and $t$.
We minimize the following equation with respect to the weight vector $w$ to infer the optimal weights:

\begin{equation}
\min_w ||\langle w, X \rangle - y||^2 + \lambda ||w||^2 \; ,
\end{equation}

\noindent where $\lambda$ is a regularization parameter that controls for overfitting of the data. The parameter $k$ defining the number of data points relevant in determining the data point $y_{t+1}$ and consequently the number of weights is a free parameter to be estimated (see Section \ref{sec:freeparams} below.

\subsection{Power Spectral Features}

\subsection{Hardness Ratio Features}

\subsection{Feature Selection and Supervised Classification}

\subsubsection{Free Parameters and Cross-validation}
label{sec:freeparams}


\section{Unsupervised Classification}


\section{Discussion}


\section{Conclusion}

\paragraph{acknowledgements}

%\bibliography{td}
%\bibliographystyle{apj}

\end{document}


