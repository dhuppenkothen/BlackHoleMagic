{
 "metadata": {
  "name": "",
  "signature": "sha256:c6a1a4a1c8cbbbb68411b7f81909d470270ef88be7f472e0c9c7884954c0eb3e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data Visualisation for GRS 1915 Light Curves\n",
      "\n",
      "Here, we're going to do some data visualisation to get a first idea of what we're dealing \n",
      "with and what kind of features should be useful. \n",
      "\n",
      "Things to do:\n",
      "- plot light curves from every class\n",
      "- plot (averaged) power spectra from every class\n",
      "- plot hardness ratios for every class (in the same plot?)\n",
      "- plot hardness ratio 2D histogram map\n",
      "- plot PCA components\n",
      "- plot funky manifold representation\n",
      "\n",
      "First, let's load some data. We're going to load full light curves, not segments, \n",
      "from the pre-classified data set for the moment. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_context(\"notebook\", font_scale=2.5, rc={\"axes.labelsize\": 26})\n",
      "plt.rc(\"font\", size=24, family=\"serif\", serif=\"Computer Sans\")\n",
      "plt.rc(\"axes\", titlesize=20, labelsize=20) \n",
      "plt.rc(\"text\", usetex=True)\n",
      "\n",
      "import numpy as np\n",
      "import cPickle as pickle\n",
      "from pandas.tools.plotting import scatter_matrix\n",
      "import pandas as pd\n",
      "\n",
      "import powerspectrum\n",
      "import grs1915_utils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/__init__.py:1312: UserWarning:  This call to matplotlib.use() has no effect\n",
        "because the backend has already been chosen;\n",
        "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\n",
        "  warnings.warn(_use_error_msg)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"grs1915_clean_label_125ms.dat\")\n",
      "d_all = pickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plotting light curves and hardness ratios from every class\n",
      "\n",
      "We're going to plot these to file, otherwise it gets too big!\n",
      "\n",
      "First, we'll extract a bunch of unique light curves, one example for each class, from \n",
      "the entire data set of classified light curves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.shuffle(d_all)\n",
      "\n",
      "states = [d[1] for d in d_all]\n",
      "unique_states = np.unique(states)\n",
      "print(\"unique states: \" + str(unique_states))\n",
      "\n",
      "d_all_reverse = [[d[1], d[0]] for d in d_all]\n",
      "\n",
      "def extract_unique(d_all):\n",
      "    seen = set()\n",
      "    d_all_unique = []\n",
      "    seen_add = seen.add\n",
      "    for d in d_all:\n",
      "        if d[1] not in seen:\n",
      "            data = d[0]\n",
      "            tseg = data[-1,0]-data[0,0]\n",
      "            if tseg < 1024.0:\n",
      "                continue\n",
      "            else:\n",
      "                seen_add(d[1])\n",
      "                d_all_unique.append(d)\n",
      "    return d_all_unique\n",
      "    \n",
      "\n",
      "d_all_unique = extract_unique(d_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unique states: ['alpha' 'beta' 'chi1' 'chi2' 'chi3' 'chi4' 'delta' 'gamma' 'kappa'\n",
        " 'lambda' 'mu' 'nu' 'phi' 'rho' 'theta']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can actually loop over the sample of unique light curves. Below, we make one plot for each class, with the light curve (left panel), the scatter plot of the two hardness ratios (middle panel) and the periodogram averaged over all segments in a given light curve (right panel)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import grs1915_utils\n",
      "\n",
      "for d in d_all_unique:\n",
      "    data = d[0]\n",
      "    state = d[1]\n",
      "    times = data[:,0]\n",
      "    tseg = times[-1]-times[0]\n",
      "    print(\"tseg: \" + str(tseg))\n",
      "    counts = data[:,1]\n",
      "    low_counts = data[:,2]\n",
      "    high_counts = data[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    \n",
      "    fig = plt.figure(figsize=(16,6))\n",
      "    ax = fig.add_subplot(1,3,1)\n",
      "    ax.plot(times, counts)\n",
      "    ax.set_xlabel(\"Time [s]\", fontsize=18)\n",
      "    ax.set_ylabel(\"Count rate [counts/s]\", fontsize=18)\n",
      "    ax.set_ylim([0.0, 12000.])\n",
      "    \n",
      "    ax2 = fig.add_subplot(1,3,2)\n",
      "    ax2.scatter(hr1, hr2)\n",
      "    ax2.set_xlim([0.3, 0.85])\n",
      "    ax2.set_ylim([0.04, 0.7])\n",
      "    ax2.set_xlabel(\"HR1 [low energy/total]\", fontsize=18)\n",
      "    ax2.set_ylabel(\"HR2 [high energy/total]\", fontsize=18)\n",
      "    \n",
      "    ax3 = fig.add_subplot(1,3,3)\n",
      "    \n",
      "    seg, labels = grs1915_utils.extract_segments([d], 256., 128.)\n",
      "    psd_avg = []\n",
      "    n_psd = len(seg)\n",
      "    ps_all = []\n",
      "    for s in seg:\n",
      "        ps = powerspectrum.PowerSpectrum(d[0][:,0], counts=d[0][:,1], norm=\"leahy\")\n",
      "        ps_all.append(ps)\n",
      "        psd_avg.append(ps.ps[1:])    \n",
      " \n",
      "    if len(ps_all) < 1:\n",
      "        print(\"No periodograms in class %s\"%state)\n",
      "        continue\n",
      "        \n",
      "    psd_avg = np.array(psd_avg)\n",
      "    psd_avg = np.sum(psd_avg, axis=0)    \n",
      "    ax3.loglog(ps_all[0].freq[1:], psd_avg, lw=2, linestyle=\"steps-mid\")  \n",
      "    ax3.set_xlabel(\"Frequency [Hz]\", fontsize=18)\n",
      "    ax3.set_ylabel(\"Averaged Leahy Power\", fontsize=18)\n",
      "    ax3.set_title(\"%i averaged periodograms\"%len(seg), fontsize=16)\n",
      "    \n",
      "    fig.suptitle(\"Light curve and Hardness ratios for class %s.\"%state, fontsize=20)\n",
      "\n",
      "    plt.savefig(\"grs1915_%s_lc.pdf\"%state, format=\"pdf\")\n",
      "    plt.close()\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tseg: 2031.75\n",
        "tseg: 3103.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 3311.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1197.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 3343.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1942.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1603.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1131.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1871.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1263.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1231.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1031.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 3295.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 1903.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tseg: 2807.75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hardness Ratio Heat Maps\n",
      "\n",
      "One way to characterise the hardness ratios is by binning them in a 2D histogram, which could then be used as a feature vector. \n",
      "\n",
      "This is what I'm going to do here. First, we need to know the lower/upper limits of the hardness ratios for all light curves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr1_all, hr2_all = [], []\n",
      "hr1_lower, hr1_upper, hr2_lower, hr2_upper = [], [], [], []\n",
      "for i,d in enumerate(d_all):\n",
      "    data = d[0]\n",
      "    times = data[:,0]\n",
      "    counts = data[:,1]\n",
      "    low_counts = data[:,2]\n",
      "    high_counts = data[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    if any(np.isnan(hr1)):\n",
      "        print(\"NaN in HR1 in light curve %i!\"%i)\n",
      "    if any(np.isnan(hr2)):\n",
      "        print(\"NaN in HR2 in light curve %i!\"%i)\n",
      "        \n",
      "    hr1_lower.append(np.min(hr1))\n",
      "    hr2_lower.append(np.min(hr2))\n",
      "    hr1_upper.append(np.max(hr1))\n",
      "    hr2_upper.append(np.max(hr2))\n",
      "    hr1_all.append(hr1)\n",
      "    hr2_all.append(hr2)\n",
      "\n",
      "hr1_min, hr1_max = np.min(hr1_lower), np.max(hr1_upper)\n",
      "hr2_min, hr2_max = np.min(hr2_lower), np.max(hr2_upper)\n",
      "\n",
      "print(\"The limits for HR1 are [%.3f, %.3f].\"%(hr1_min, hr1_max))\n",
      "print(\"The limits for HR2 are [%.3f, %.3f].\"%(hr2_min, hr2_max))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The limits for HR1 are [0.292, 0.820].\n",
        "The limits for HR2 are [0.046, 0.708].\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can make scatter plots and 2D histograms of all states with the same resolution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in d_all_unique:\n",
      "    state = d[1]\n",
      "    data = d[0]\n",
      "    times = data[:,0]\n",
      "    counts = data[:,1]\n",
      "    low_counts = data[:,2]\n",
      "    high_counts = data[:,3]\n",
      "    hr1 = low_counts/counts\n",
      "    hr2 = high_counts/counts\n",
      "    h, xedges, yedges = np.histogram2d(hr1, hr2, bins=30, \n",
      "                                       range=[[hr1_min, hr1_max], [hr2_min, hr2_max]])\n",
      "    h = np.rot90(h)\n",
      "    h = np.flipud(h)\n",
      "\n",
      "    fig = plt.figure(figsize=(12,6))\n",
      "    plt.subplot(1,2,1)\n",
      "    plt.scatter(hr1, hr2)\n",
      "    plt.axis([hr1_min, hr1_max, hr2_min, hr2_max])\n",
      "    plt.subplot(1,2,2)\n",
      "    plt.pcolormesh(xedges,yedges,h,cmap='BuPu' )\n",
      "    plt.axis([hr1_min, hr1_max, hr2_min, hr2_max])\n",
      "\n",
      "    plt.suptitle(\"HR Ratio scatter plot and 2D histogram for state %s.\"%state, fontsize=20)\n",
      "\n",
      "    plt.savefig(\"grs1915_%s_2dhist.pdf\"%state, format=\"pdf\")\n",
      "    plt.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems like it might be a good feature to use. It will be very sparse, which may or may not screw up some methods. I should ask someone about that!\n",
      "\n",
      "## Time Series Features\n",
      "\n",
      "Let's look at some time series features next. \n",
      "\n",
      "First, let's look at the average count rate and the total variance in the light curve for different classes, as well as some power spectral estimates. \n",
      "\n",
      "We're going to start by extracting segments to augment our data set. Right now, we're not concerned with training versus test sets, just with properties of the entire data set, so we'll work on the entire set of labelled data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seg, labels = grs1915_utils.extract_segments(d_all, seg_length=256., overlap=64.)\n",
      "labels_unique,labels_numeric = np.unique(labels, return_inverse=True) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to look at the mean count rate, median count rate and total variance in the light curve. For all of the stuff below, we're looking at the 2-13 keV light curve only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fmu = np.array([np.mean(s) for s in seg])\n",
      "fmed = np.array([np.median(s) for s in seg])\n",
      "fvar = np.array([np.var(s) for s in seg])\n",
      "\n",
      "fig = plt.figure(figsize=(16,8))\n",
      "ax = fig.add_subplot(1,2,1)\n",
      "ax.scatter(fmu, fvar, c=plt.cm.Spectral(labels_numeric / 14.))\n",
      "ax.set_xscale(\"log\")\n",
      "ax.set_yscale(\"log\")\n",
      "ax.set_xlabel(\"Mean Count rate [counts/s]\", fontsize=20)\n",
      "ax.set_ylabel(\"Total variance in the light curve\", fontsize=20)\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2)\n",
      "ax2.scatter(fmed, fvar, c=plt.cm.Spectral(labels_numeric / 14.))\n",
      "ax2.set_xscale(\"log\")\n",
      "ax2.set_yscale(\"log\")\n",
      "ax2.set_xlabel(\"Median Count rate [counts/s]\", fontsize=20)\n",
      "ax2.set_ylabel(\"Total variance in the light curve\", fontsize=20)\n",
      "\n",
      "plt.savefig(\"grs1915_mean_median_countrate.pdf\", format=\"pdf\")\n",
      "plt.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm not sure whether that's worth exploring yet, but there seems to be more variety in the median compared to the mean, so perhaps that is a better estimator for what we're looking for.\n",
      "\n",
      "Next, let's look at some power spectral statistics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pcb = {\"pa_min\":0.0039, \"pa_max\":0.031, \n",
      "                           \"pb_min\":0.031, \"pb_max\":0.25,\n",
      "                           \"pc_min\":0.25, \"pc_max\":2.0,\n",
      "                           \"pd_min\":2.0, \"pd_max\":16.0}\n",
      "\n",
      "\n",
      "\n",
      "fmax_all, psd_a, psd_b, psd_c, psd_d = [], [], [], [], []\n",
      "\n",
      "for s in seg:\n",
      "    dt = s[1:,0] - s[:-1,0]\n",
      "    dt = np.min(dt)\n",
      "    ps = powerspectrum.PowerSpectrum(s[:,0], counts=s[:,1]*dt, norm=\"rms\")\n",
      "\n",
      "    if len(ps.freq) < 1024:\n",
      "        print(\"i: \" + str(i))\n",
      "        print(\"j: \" + str(j))\n",
      "        print(len(ps.freq))\n",
      "        print(ps.freq[0])\n",
      "        print(ps.freq[-1])\n",
      "\n",
      "    ## find frequency with maximum power, exclude 0th frequency\n",
      "    fmax_ind = np.where(ps.ps[1:] == np.max(ps.ps[1:]))[0]+1\n",
      "    fmax_all.append(ps.freq[fmax_ind[0]])\n",
      "\n",
      "    ## find power in spectral bands for power-colours\n",
      "    pa_min_freq = ps.freq.searchsorted(pcb[\"pa_min\"])\n",
      "    pa_max_freq = ps.freq.searchsorted(pcb[\"pa_max\"])\n",
      "\n",
      "    pb_min_freq = ps.freq.searchsorted(pcb[\"pb_min\"])\n",
      "    pb_max_freq = ps.freq.searchsorted(pcb[\"pb_max\"])\n",
      "\n",
      "    pc_min_freq = ps.freq.searchsorted(pcb[\"pc_min\"])\n",
      "    pc_max_freq = ps.freq.searchsorted(pcb[\"pc_max\"])\n",
      "\n",
      "    pd_min_freq = ps.freq.searchsorted(pcb[\"pd_min\"])\n",
      "    pd_max_freq = ps.freq.searchsorted(pcb[\"pd_max\"])\n",
      "\n",
      "    psd_a.append(np.sum(ps.ps[pa_min_freq:pa_max_freq]))\n",
      "    psd_b.append(np.sum(ps.ps[pb_min_freq:pb_max_freq]))\n",
      "    psd_c.append(np.sum(ps.ps[pc_min_freq:pc_max_freq]))\n",
      "    psd_d.append(np.sum(ps.ps[pd_min_freq:pd_max_freq]))\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/danielahuppenkothen/repositories/giantflare-paper/code/powerspectrum.py:51: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  elif not lc == None and not counts == None:\n",
        "/Users/danielahuppenkothen/repositories/giantflare-paper/code/lightcurve.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if counts == None:\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## define power colours: PC/PA and PB/PD\n",
      "psd_a = np.array(psd_a)\n",
      "psd_b = np.array(psd_b)\n",
      "psd_c = np.array(psd_c)\n",
      "psd_d = np.array(psd_d)\n",
      "\n",
      "pc1 = psd_c/psd_a\n",
      "pc2 = psd_b/psd_d\n",
      "\n",
      "## Let's make a scatter matrix\n",
      "\n",
      "ts_features = np.transpose(np.array([fmax_all, psd_a, psd_b, psd_c, psd_d, pc1, pc2]))\n",
      "\n",
      "#df = pd.DataFrame(ts_features)\n",
      "#scatter_matrix(df)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "axis_label = [\"$F_{\\mathrm{max}}$\", \"PSD A\", \"PSD B\", \"PSD C\", \"PSD D\", \"PC 1\", \"PC 2\"]\n",
      "fig = plt.figure(figsize=(30,30))\n",
      "n_features = len(np.transpose(ts_features))\n",
      "for i,f1 in enumerate(np.transpose(ts_features)):\n",
      "    for j,f2 in enumerate(np.transpose(ts_features)):\n",
      "        ax = fig.add_subplot(n_features,n_features,(i*n_features)+j+1)\n",
      "        if i == j:\n",
      "            ax.hist(np.log10(f1), bins=50)\n",
      "            ax.set_xlabel(axis_label[i])\n",
      "            ax.set_ylabel(\"N(%s)\"%axis_label[i])\n",
      "        else:\n",
      "            ax.scatter(np.log10(f1),np.log10(f2), c=plt.cm.Spectral(labels_numeric / 14.))\n",
      "            ax.set_xlabel(axis_label[i])\n",
      "            ax.set_ylabel(axis_label[j])\n",
      "\n",
      "plt.savefig(\"grs1915_psd_features_scatter.pdf\", format=\"pdf\")\n",
      "plt.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making A Crazy-ass Manifold Plot\n",
      "\n",
      "This is going to be crazy and probably break my computer, but we'll do it anyway.\n",
      "We will use something called \"t-distributed Stochastic Neighbor Embedding\". \n",
      "\n",
      "Let's try it on all the non-hardness ratio features first, so that we don't try 910 features\n",
      "all at once! Also, we're going to normalise the features, such that they don't differ by orders of magnitude. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = np.array([fmu, fmed, fvar, fmax_all, psd_a, psd_b, psd_c, psd_d, pc1, pc2])\n",
      "features = np.transpose(features)\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "scaler = StandardScaler().fit(features)\n",
      "f_scaled = scaler.transform(features)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import manifold\n",
      "\n",
      "tsne = manifold.TSNE()\n",
      "asdf = tsne.fit_transform(f_scaled)\n",
      "\n",
      "plt.figure(figsize=(20, 20))\n",
      "plt.scatter(asdf[:, 0], asdf[:, 1], c=plt.cm.Spectral(labels_numeric / 14.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}